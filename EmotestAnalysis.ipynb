{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 755.967641,
      "end_time": "2021-07-04T09:04:43.274041",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-04T08:52:07.306400",
      "version": "2.3.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ee76147aa614e759d313bab4ff18136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fff731170a424388b59abb7bff32b3a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cdee75fc7d4045089445c7d5266baabe",
              "IPY_MODEL_0ae98fd546b04efa989687ee5a2644ad",
              "IPY_MODEL_24dc28a2e2a04606bf57f3d5dc369f69"
            ]
          }
        },
        "fff731170a424388b59abb7bff32b3a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdee75fc7d4045089445c7d5266baabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63e5f9fc15e24f14a1e53aa3b33ec062",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ec206be73e04aea88b9684b3a28934d"
          }
        },
        "0ae98fd546b04efa989687ee5a2644ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce52bbce3e0845129edf592f3ff62823",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_543d045a705143ed9f178e3af6834c57"
          }
        },
        "24dc28a2e2a04606bf57f3d5dc369f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a8003af58b6944a7807496bd790d622a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 878k/878k [00:00&lt;00:00, 2.01MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0cd952b66c7a49a583804e7fc821b500"
          }
        },
        "63e5f9fc15e24f14a1e53aa3b33ec062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ec206be73e04aea88b9684b3a28934d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce52bbce3e0845129edf592f3ff62823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "543d045a705143ed9f178e3af6834c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8003af58b6944a7807496bd790d622a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0cd952b66c7a49a583804e7fc821b500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a2647a76c2e489fbaf537b397dd8f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5cdc263b29514087a94fff269b8f9ad8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05a9c88469e14feb86ce179337decb29",
              "IPY_MODEL_9d8274220e2b4209ad53846aa4c90fd1",
              "IPY_MODEL_a00c3404ceb346c48a61e51818076308"
            ]
          }
        },
        "5cdc263b29514087a94fff269b8f9ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05a9c88469e14feb86ce179337decb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a33e8766a5ba472fbae89398d8fc3586",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7dedde2012294ad29321764c0838f7a9"
          }
        },
        "9d8274220e2b4209ad53846aa4c90fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50d82982c49840feb39cb9656accd23e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bea73da5b264008bb7736174fe326d4"
          }
        },
        "a00c3404ceb346c48a61e51818076308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63a13714d77a4efcb2139039f009c1f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 655kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c269466f25774f298e270dd770b65395"
          }
        },
        "a33e8766a5ba472fbae89398d8fc3586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7dedde2012294ad29321764c0838f7a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50d82982c49840feb39cb9656accd23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bea73da5b264008bb7736174fe326d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63a13714d77a4efcb2139039f009c1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c269466f25774f298e270dd770b65395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7f15e2c7a6345088cb43f36f8e659b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a144336e3304e15b3ec80f656ffd46a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e5cfbc013f2a467da223c7b6e80fbb08",
              "IPY_MODEL_17c2adad8a9c40a6baf3f1e4afc13031",
              "IPY_MODEL_7a47be011a314fe5b98fd0e3cf6f2243"
            ]
          }
        },
        "2a144336e3304e15b3ec80f656ffd46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5cfbc013f2a467da223c7b6e80fbb08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a924a6c33bf742e0b05daab093fb3bc4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2f0a9993e8747db90d1c8f00a0f7930"
          }
        },
        "17c2adad8a9c40a6baf3f1e4afc13031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b662cc81e185477b85734dd31f531c23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_724051a3253b497c9f7c23120ae125d2"
          }
        },
        "7a47be011a314fe5b98fd0e3cf6f2243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_30cbb254966247979e24ec5949bd18a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 2.10MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_126d9c96e3aa494299bd761af8a17f1e"
          }
        },
        "a924a6c33bf742e0b05daab093fb3bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2f0a9993e8747db90d1c8f00a0f7930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b662cc81e185477b85734dd31f531c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "724051a3253b497c9f7c23120ae125d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30cbb254966247979e24ec5949bd18a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "126d9c96e3aa494299bd761af8a17f1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a53918f45fce42e98dc9c4f3df9616ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c7c5bfe2c9f84a198ccf97d9cded6296",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0b69deb14754426eab719f2b5dc41ca9",
              "IPY_MODEL_4afa9c93ed214892aa457ea46fd8683c",
              "IPY_MODEL_7f9cb395b0234e4b90d2cc47a17fd972"
            ]
          }
        },
        "c7c5bfe2c9f84a198ccf97d9cded6296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b69deb14754426eab719f2b5dc41ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66fd95e8561f44269b3417989399a920",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b967696dcd649e3997a21d62777a4f4"
          }
        },
        "4afa9c93ed214892aa457ea46fd8683c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e94f1d4f729d439c84f04c13c4ff4040",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bdd862f1f08844cbae4ef503ae755301"
          }
        },
        "7f9cb395b0234e4b90d2cc47a17fd972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bc672b88566c4813b1597e47af040025",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 11.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73aa5c08620e44998bb1e20782f8f34d"
          }
        },
        "66fd95e8561f44269b3417989399a920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b967696dcd649e3997a21d62777a4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e94f1d4f729d439c84f04c13c4ff4040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bdd862f1f08844cbae4ef503ae755301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc672b88566c4813b1597e47af040025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73aa5c08620e44998bb1e20782f8f34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b03b80e208ad45cb92600d2ade828011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b4c487227d86444393d3d470c8833944",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c207df892cb44832ac6e0848a8595d39",
              "IPY_MODEL_d963aa0a89274418a7b23b05fb22d322",
              "IPY_MODEL_ae2a2550767d4048bb5f1ebc724f60bd"
            ]
          }
        },
        "b4c487227d86444393d3d470c8833944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c207df892cb44832ac6e0848a8595d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f2aae9a90e140d191afbe7c2cd98ad3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0dc3706ffb84eeb8f02687c02549245"
          }
        },
        "d963aa0a89274418a7b23b05fb22d322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78614a98cdb947c4b1dc1e9e40cbf022",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 657434796,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 657434796,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1255e1751b3b4d96bc39f0f14a6b9ab9"
          }
        },
        "ae2a2550767d4048bb5f1ebc724f60bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a9b9452e0a243a6a4ed342cfa21bff6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 627M/627M [00:32&lt;00:00, 9.73MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_355a5ba2738c46b999c1c4624da52310"
          }
        },
        "7f2aae9a90e140d191afbe7c2cd98ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0dc3706ffb84eeb8f02687c02549245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78614a98cdb947c4b1dc1e9e40cbf022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1255e1751b3b4d96bc39f0f14a6b9ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a9b9452e0a243a6a4ed342cfa21bff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "355a5ba2738c46b999c1c4624da52310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dated-graphic"
      },
      "source": [
        "# Import Libraries"
      ],
      "id": "dated-graphic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hawaiian-dating",
        "outputId": "6836d501-3f64-4a5e-8fdb-47e5947601c6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import statistics"
      ],
      "id": "hawaiian-dating",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 11.9 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 44.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85454 sha256=7e33a7bf19f6152cee03f55e893ba3556373da4997b04fbdaf49603269df1cdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4_aHofmwboT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab901f0a-8dc1-4a2a-f4f3-a2ef396b8af6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "r4_aHofmwboT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "accepted-carbon",
        "outputId": "ce2289f9-36c9-4389-a227-d7532ec717fa"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/train_clean.csv')\n",
        "val_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/val_clean.csv')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/test_clean.csv')\n",
        "val_data.head()"
      ],
      "id": "accepted-carbon",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a47e331e-c6dc-4512-b920-cfb8863acafe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clean_text</th>\n",
              "      <th>admiration</th>\n",
              "      <th>amusement</th>\n",
              "      <th>anger</th>\n",
              "      <th>annoyance</th>\n",
              "      <th>approval</th>\n",
              "      <th>caring</th>\n",
              "      <th>confusion</th>\n",
              "      <th>curiosity</th>\n",
              "      <th>desire</th>\n",
              "      <th>disappointment</th>\n",
              "      <th>disapproval</th>\n",
              "      <th>disgust</th>\n",
              "      <th>embarrassment</th>\n",
              "      <th>excitement</th>\n",
              "      <th>fear</th>\n",
              "      <th>gratitude</th>\n",
              "      <th>grief</th>\n",
              "      <th>joy</th>\n",
              "      <th>love</th>\n",
              "      <th>nervousness</th>\n",
              "      <th>optimism</th>\n",
              "      <th>pride</th>\n",
              "      <th>realization</th>\n",
              "      <th>relief</th>\n",
              "      <th>remorse</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is this in new orleans ?? i really feel like t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you know the answer man you are programmed to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i have never been this sad in my life !</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the economy is heavily controlled and subsidiz...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>he could have easily taken a real camera from ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a47e331e-c6dc-4512-b920-cfb8863acafe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a47e331e-c6dc-4512-b920-cfb8863acafe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a47e331e-c6dc-4512-b920-cfb8863acafe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          Clean_text  ...  neutral\n",
              "0  is this in new orleans ?? i really feel like t...  ...        1\n",
              "1  you know the answer man you are programmed to ...  ...        1\n",
              "2            i have never been this sad in my life !  ...        0\n",
              "3  the economy is heavily controlled and subsidiz...  ...        1\n",
              "4  he could have easily taken a real camera from ...  ...        0\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conditional-fluid"
      },
      "source": [
        "**Check for null values in train, val and test dataset**"
      ],
      "id": "conditional-fluid"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "placed-oxide",
        "outputId": "bc7d942f-aa39-40e6-f34b-e5e76405531d"
      },
      "source": [
        "# Check for null values in train, val and test dataset\n",
        "data = {'Train Data': train_data, 'Validation Data': val_data, 'Test Data': test_data}\n",
        "for temp in data:\n",
        "    print(temp)\n",
        "    print(data[temp].isnull().sum())\n",
        "    print('*'*20)"
      ],
      "id": "placed-oxide",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data\n",
            "Clean_text        14\n",
            "admiration         0\n",
            "amusement          0\n",
            "anger              0\n",
            "annoyance          0\n",
            "approval           0\n",
            "caring             0\n",
            "confusion          0\n",
            "curiosity          0\n",
            "desire             0\n",
            "disappointment     0\n",
            "disapproval        0\n",
            "disgust            0\n",
            "embarrassment      0\n",
            "excitement         0\n",
            "fear               0\n",
            "gratitude          0\n",
            "grief              0\n",
            "joy                0\n",
            "love               0\n",
            "nervousness        0\n",
            "optimism           0\n",
            "pride              0\n",
            "realization        0\n",
            "relief             0\n",
            "remorse            0\n",
            "sadness            0\n",
            "surprise           0\n",
            "neutral            0\n",
            "dtype: int64\n",
            "********************\n",
            "Validation Data\n",
            "Clean_text        1\n",
            "admiration        0\n",
            "amusement         0\n",
            "anger             0\n",
            "annoyance         0\n",
            "approval          0\n",
            "caring            0\n",
            "confusion         0\n",
            "curiosity         0\n",
            "desire            0\n",
            "disappointment    0\n",
            "disapproval       0\n",
            "disgust           0\n",
            "embarrassment     0\n",
            "excitement        0\n",
            "fear              0\n",
            "gratitude         0\n",
            "grief             0\n",
            "joy               0\n",
            "love              0\n",
            "nervousness       0\n",
            "optimism          0\n",
            "pride             0\n",
            "realization       0\n",
            "relief            0\n",
            "remorse           0\n",
            "sadness           0\n",
            "surprise          0\n",
            "neutral           0\n",
            "dtype: int64\n",
            "********************\n",
            "Test Data\n",
            "Clean_text        2\n",
            "admiration        0\n",
            "amusement         0\n",
            "anger             0\n",
            "annoyance         0\n",
            "approval          0\n",
            "caring            0\n",
            "confusion         0\n",
            "curiosity         0\n",
            "desire            0\n",
            "disappointment    0\n",
            "disapproval       0\n",
            "disgust           0\n",
            "embarrassment     0\n",
            "excitement        0\n",
            "fear              0\n",
            "gratitude         0\n",
            "grief             0\n",
            "joy               0\n",
            "love              0\n",
            "nervousness       0\n",
            "optimism          0\n",
            "pride             0\n",
            "realization       0\n",
            "relief            0\n",
            "remorse           0\n",
            "sadness           0\n",
            "surprise          0\n",
            "neutral           0\n",
            "dtype: int64\n",
            "********************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNX7anhoRQJ5"
      },
      "source": [
        "# eliminate null value\n",
        "train_data = train_data[ ~(train_data.Clean_text.isnull()) ]\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "val_data = val_data[ ~(val_data.Clean_text.isnull()) ]\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "test_data = test_data[ ~(test_data.Clean_text.isnull()) ]\n",
        "test_data = test_data.reset_index(drop=True)"
      ],
      "id": "sNX7anhoRQJ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyX6qkik4Zaw"
      },
      "source": [
        "# Loading emotion labels\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Data/emotions.txt\", \"r\") as file:\n",
        "  Lable_names = file.read().split(\"\\n\")"
      ],
      "id": "AyX6qkik4Zaw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc6uSKqs5cIC"
      },
      "source": [
        "# Creating train, validation and test variables\n",
        "y_train = train_data.loc[:, Lable_names].values.astype(float)\n",
        "y_val = val_data.loc[:, Lable_names].values.astype(float)\n",
        "y_test = test_data.loc[:, Lable_names].values.astype(float)"
      ],
      "id": "Bc6uSKqs5cIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "curious-parcel"
      },
      "source": [
        "# Encoding"
      ],
      "id": "curious-parcel"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hollywood-plymouth"
      },
      "source": [
        "There is a very helpful function called encode_plus provided in the Tokenizer class. It can seamlessly perform the following operations:\n",
        "\n",
        "* Tokenize the text and Add special tokens - [CLS] and [SEP]\n",
        "* create input IDs\n",
        "* Pad the sentences to a maximum length\n",
        "* Create attention masks for the above PAD tokens\n",
        "\n",
        " RoBERTa uses byte-level Byte-Pair Encoding (BPE) in contrast to BERT’s character-level BPE."
      ],
      "id": "hollywood-plymouth"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EazIF1Zt8XGt",
        "outputId": "a8e8150a-fc72-4f7f-8192-9f6154b2af0c"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "EazIF1Zt8XGt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 37.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seven-suspension",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "3ee76147aa614e759d313bab4ff18136",
            "fff731170a424388b59abb7bff32b3a7",
            "cdee75fc7d4045089445c7d5266baabe",
            "0ae98fd546b04efa989687ee5a2644ad",
            "24dc28a2e2a04606bf57f3d5dc369f69",
            "63e5f9fc15e24f14a1e53aa3b33ec062",
            "9ec206be73e04aea88b9684b3a28934d",
            "ce52bbce3e0845129edf592f3ff62823",
            "543d045a705143ed9f178e3af6834c57",
            "a8003af58b6944a7807496bd790d622a",
            "0cd952b66c7a49a583804e7fc821b500",
            "3a2647a76c2e489fbaf537b397dd8f95",
            "5cdc263b29514087a94fff269b8f9ad8",
            "05a9c88469e14feb86ce179337decb29",
            "9d8274220e2b4209ad53846aa4c90fd1",
            "a00c3404ceb346c48a61e51818076308",
            "a33e8766a5ba472fbae89398d8fc3586",
            "7dedde2012294ad29321764c0838f7a9",
            "50d82982c49840feb39cb9656accd23e",
            "8bea73da5b264008bb7736174fe326d4",
            "63a13714d77a4efcb2139039f009c1f7",
            "c269466f25774f298e270dd770b65395",
            "c7f15e2c7a6345088cb43f36f8e659b3",
            "2a144336e3304e15b3ec80f656ffd46a",
            "e5cfbc013f2a467da223c7b6e80fbb08",
            "17c2adad8a9c40a6baf3f1e4afc13031",
            "7a47be011a314fe5b98fd0e3cf6f2243",
            "a924a6c33bf742e0b05daab093fb3bc4",
            "c2f0a9993e8747db90d1c8f00a0f7930",
            "b662cc81e185477b85734dd31f531c23",
            "724051a3253b497c9f7c23120ae125d2",
            "30cbb254966247979e24ec5949bd18a1",
            "126d9c96e3aa494299bd761af8a17f1e",
            "a53918f45fce42e98dc9c4f3df9616ae",
            "c7c5bfe2c9f84a198ccf97d9cded6296",
            "0b69deb14754426eab719f2b5dc41ca9",
            "4afa9c93ed214892aa457ea46fd8683c",
            "7f9cb395b0234e4b90d2cc47a17fd972",
            "66fd95e8561f44269b3417989399a920",
            "4b967696dcd649e3997a21d62777a4f4",
            "e94f1d4f729d439c84f04c13c4ff4040",
            "bdd862f1f08844cbae4ef503ae755301",
            "bc672b88566c4813b1597e47af040025",
            "73aa5c08620e44998bb1e20782f8f34d"
          ]
        },
        "outputId": "64a8acfd-2389-4a1d-d31b-6e978c4c53e6"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
      ],
      "id": "seven-suspension",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ee76147aa614e759d313bab4ff18136",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a2647a76c2e489fbaf537b397dd8f95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7f15e2c7a6345088cb43f36f8e659b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a53918f45fce42e98dc9c4f3df9616ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suburban-identifier"
      },
      "source": [
        "def roberta_encode(data,maximum_length) :\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for i in range(len(data.Clean_text)):\n",
        "      encoded = tokenizer.encode_plus(data.Clean_text[i],\n",
        "                                      add_special_tokens=True,\n",
        "                                      max_length=maximum_length,\n",
        "                                      pad_to_max_length=True,\n",
        "                                      return_attention_mask=True,)\n",
        "      input_ids.append(encoded['input_ids'])\n",
        "      attention_masks.append(encoded['attention_mask'])\n",
        "  return np.array(input_ids),np.array(attention_masks)"
      ],
      "id": "suburban-identifier",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFoEwNls7ig_",
        "outputId": "060dba4a-9aa8-4ad5-a44c-8a634e5dc3e1"
      },
      "source": [
        "#train_data['content_no_punctuation'] = train_data['content_no_punctuation'].str.count(' ')\n",
        "train_data['Clean_text'].describe()"
      ],
      "id": "cFoEwNls7ig_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count         43396\n",
              "unique        42977\n",
              "top       thank you\n",
              "freq             19\n",
              "Name: Clean_text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thorough-invite",
        "outputId": "1267ddb6-41c9-4f2b-d13d-964fff8d74a7"
      },
      "source": [
        "max_len = max([len(x.split()) for x in train_data['Clean_text']])\n",
        "# max_len = 15\n",
        "train_input_ids,train_attention_masks = roberta_encode(train_data,max_len)\n",
        "test_input_ids,test_attention_masks = roberta_encode(test_data,max_len)\n",
        "val_input_ids,val_attention_masks = roberta_encode(val_data,max_len)"
      ],
      "id": "thorough-invite",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRdv-e5BQ3yR",
        "outputId": "0b126687-ada0-484e-df50-bf0a91b1b888"
      },
      "source": [
        "max_len"
      ],
      "id": "HRdv-e5BQ3yR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pnazle9Eqfw",
        "outputId": "0fb648c4-7b87-4a3b-e8af-f384a022f1e2"
      },
      "source": [
        "pd.options.display.max_colwidth = 500\n",
        "train_data.iloc[80]"
      ],
      "id": "6pnazle9Eqfw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Clean_text        oh forgive u for trying to make an exciting atmosphere at our homecourt\n",
              "admiration                                                                              0\n",
              "amusement                                                                               0\n",
              "anger                                                                                   0\n",
              "annoyance                                                                               0\n",
              "approval                                                                                0\n",
              "caring                                                                                  0\n",
              "confusion                                                                               0\n",
              "curiosity                                                                               0\n",
              "desire                                                                                  0\n",
              "disappointment                                                                          0\n",
              "disapproval                                                                             0\n",
              "disgust                                                                                 0\n",
              "embarrassment                                                                           0\n",
              "excitement                                                                              1\n",
              "fear                                                                                    0\n",
              "gratitude                                                                               0\n",
              "grief                                                                                   0\n",
              "joy                                                                                     0\n",
              "love                                                                                    0\n",
              "nervousness                                                                             0\n",
              "optimism                                                                                0\n",
              "pride                                                                                   0\n",
              "realization                                                                             0\n",
              "relief                                                                                  0\n",
              "remorse                                                                                 0\n",
              "sadness                                                                                 0\n",
              "surprise                                                                                0\n",
              "neutral                                                                                 0\n",
              "Name: 80, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObAWUTapskfS"
      },
      "source": [
        "# **Making an attention layer**"
      ],
      "id": "ObAWUTapskfS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMVQva6dsuia"
      },
      "source": [
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights"
      ],
      "id": "JMVQva6dsuia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making an self attention layer**"
      ],
      "metadata": {
        "id": "DUgjDmuQUJyD"
      },
      "id": "DUgjDmuQUJyD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXuV5yZbh1h0"
      },
      "source": [
        "# pip install keras-self-attention"
      ],
      "id": "PXuV5yZbh1h0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras_self_attention import SeqSelfAttention"
      ],
      "metadata": {
        "id": "q99PuGPGVhEJ"
      },
      "id": "q99PuGPGVhEJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "passive-surveillance"
      },
      "source": [
        "# Create Model"
      ],
      "id": "passive-surveillance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sustained-perfume"
      },
      "source": [
        "**How RoBERTa is better than BERT ??**\n",
        "\n",
        "Changes in Pre-Training:\n",
        "* without NSP objective\n",
        "* with dynamic mask generation\n",
        "\n",
        "Changes in Data:\n",
        "* Trained on more data (16GB BERT vs 160GB RoBERTa)\n",
        "* Trained on large batches\n",
        "\n"
      ],
      "id": "sustained-perfume"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4C93V1yz5Yp"
      },
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense , Concatenate , Flatten\n",
        "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional"
      ],
      "id": "x4C93V1yz5Yp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VUhQRfler2y"
      },
      "source": [
        "from keras.layers import Dropout"
      ],
      "id": "_VUhQRfler2y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQv8htNSC7R8"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from keras import backend as K"
      ],
      "id": "QQv8htNSC7R8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbGRXhR0E3vP"
      },
      "source": [
        "# Function for calculating multilabel class weights\n",
        "def calculating_class_weights(y_true):\n",
        "    number_dim = np.shape(y_true)[1]\n",
        "    weights = np.empty([number_dim, 2])\n",
        "    for i in range(number_dim):\n",
        "        weights[i] = compute_class_weight(class_weight='balanced', classes=[0.,1.], y=y_true[:, i])   # n_samples / (n_classes * np.bincount(y))\n",
        "    return weights\n",
        "\n",
        "class_weights = calculating_class_weights(y_train)\n",
        "\n",
        "\n",
        "# Custom loss function for multilabel\n",
        "def get_weighted_loss(weights):\n",
        "    def weighted_loss(y_true, y_pred):\n",
        "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
        "    return weighted_loss\n",
        "\n",
        "\n",
        "# Set loss\n",
        "loss = get_weighted_loss(class_weights)"
      ],
      "id": "BbGRXhR0E3vP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1SS6C2UzNBJ",
        "outputId": "13460316-a9c2-4cb9-a166-4e974ce68339"
      },
      "source": [
        "class_weights"
      ],
      "id": "e1SS6C2UzNBJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0.55259003,   5.25375303],\n",
              "       [  0.52833037,   9.32445208],\n",
              "       [  0.51873102,  13.8468411 ],\n",
              "       [  0.53017642,   8.78461538],\n",
              "       [  0.53632252,   7.38278326],\n",
              "       [  0.51284597,  19.96136155],\n",
              "       [  0.51627486,  15.86111111],\n",
              "       [  0.52658658,   9.90324053],\n",
              "       [  0.5074962 ,  33.85023401],\n",
              "       [  0.5150616 ,  17.09850276],\n",
              "       [  0.52443564,  10.73095945],\n",
              "       [  0.5092949 ,  27.39646465],\n",
              "       [  0.50351565,  71.61056106],\n",
              "       [  0.51002515,  25.43728019],\n",
              "       [  0.50696262,  36.40604027],\n",
              "       [  0.53267541,   8.15101427],\n",
              "       [  0.50088876, 281.79220779],\n",
              "       [  0.51730879,  14.94352617],\n",
              "       [  0.52524812,  10.40172579],\n",
              "       [  0.50189674, 132.30487805],\n",
              "       [  0.5189047 ,  13.72422517],\n",
              "       [  0.5012822 , 195.47747748],\n",
              "       [  0.51312491,  19.54774775],\n",
              "       [  0.50176907, 141.81699346],\n",
              "       [  0.50635924,  39.81284404],\n",
              "       [  0.51575945,  16.36349925],\n",
              "       [  0.5125189 ,  20.46981132],\n",
              "       [  0.74336223,   1.52727529]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSZ-Wk060oEg",
        "outputId": "47727557-cf9d-4092-8ff3-f7343c410ef5"
      },
      "source": [
        "a = [0,0,0,1,0,1]\n",
        "np.bincount(a)"
      ],
      "id": "hSZ-Wk060oEg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqnlbKHR0YyU",
        "outputId": "3262bf96-6b8a-419a-b01d-c12c05aa521f"
      },
      "source": [
        "y_train[:,0].shape"
      ],
      "id": "PqnlbKHR0YyU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43396,)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iru3uVM8998B",
        "outputId": "ec6535ed-b1cd-46e6-8cf5-bc121ec30f5c"
      },
      "source": [
        "K.binary_crossentropy([1. , 0.], [0.8 , 0.5])"
      ],
      "id": "iru3uVM8998B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.22314338, 0.69314694], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvHKJ1QYwLHL",
        "outputId": "d0afb5c5-f08d-447c-ff19-f5a5b8ace238"
      },
      "source": [
        "!pip install keras-self-attention\n",
        "import keras\n",
        "from keras_self_attention import SeqSelfAttention"
      ],
      "id": "DvHKJ1QYwLHL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=c0c7b238bc2de931e8317fd0a542408f5fb390c995488ae3a7471c85fd23f4fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "b03b80e208ad45cb92600d2ade828011",
            "b4c487227d86444393d3d470c8833944",
            "c207df892cb44832ac6e0848a8595d39",
            "d963aa0a89274418a7b23b05fb22d322",
            "ae2a2550767d4048bb5f1ebc724f60bd",
            "7f2aae9a90e140d191afbe7c2cd98ad3",
            "a0dc3706ffb84eeb8f02687c02549245",
            "78614a98cdb947c4b1dc1e9e40cbf022",
            "1255e1751b3b4d96bc39f0f14a6b9ab9",
            "8a9b9452e0a243a6a4ed342cfa21bff6",
            "355a5ba2738c46b999c1c4624da52310"
          ]
        },
        "id": "heated-wrist",
        "outputId": "a84f24dc-23af-4e39-d170-0d7c90302fa4"
      },
      "source": [
        "from transformers import TFRobertaModel\n",
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
      ],
      "id": "heated-wrist",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b03b80e208ad45cb92600d2ade828011",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "economic-elements"
      },
      "source": [
        "def create_model(bert_model, max_len):\n",
        "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
        "\n",
        "    output = bert_model([input_ids,attention_masks])\n",
        "\n",
        "    output = output[0]\n",
        "\n",
        "    # output = output[1]  ERROR\n",
        "\n",
        "    # print(len(output[0]))\n",
        "    # print(output[0].shape)\n",
        "    # print(output[1].shape)\n",
        "\n",
        "    # output = SeqSelfAttention(\n",
        "    #                           attention_activation='softmax'\n",
        "                              # attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                              # attention_activation=None,\n",
        "                              # kernel_regularizer=keras.regularizers.l2(1e-6),\n",
        "                              # use_attention_bias=False\n",
        "                              # )(output)\n",
        "\n",
        "    # output = tf.expand_dims(output, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # l_cov1= Conv1D(128, 5, activation='relu')(output)\n",
        "    # l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "    # l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
        "    # l_pool2 = MaxPooling1D(5)(l_cov2)\n",
        "    # l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
        "    # l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
        "    # l_flat = Flatten()(l_pool3)\n",
        "    # l_dense = Dense(128, activation='relu')(l_flat)\n",
        "    # output = Dense(28, activation='softmax')(l_dense)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    (lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(units=100,\n",
        "                                                                            return_sequences=True,\n",
        "                                                                            return_state=True,\n",
        "                                                                            recurrent_dropout=0.1))(output)\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "    # print(state_h.shape)\n",
        "\n",
        "    context_vector, attention_weights = Attention(30)(lstm, state_h)\n",
        "\n",
        "    output = tf.keras.layers.Dense(28, activation='softmax')(context_vector)\n",
        "\n",
        "\n",
        "    # output = tf.keras.layers.Dense(28, activation='softmax')(output)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
        "    model.compile(Adam(learning_rate=1e-5),\n",
        "                #  loss='categorical_crossentropy',\n",
        "                 loss=loss,\n",
        "                 metrics=['accuracy'])\n",
        "    return model"
      ],
      "id": "economic-elements",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pointed-repeat",
        "outputId": "6b5457af-f86e-4e4f-e0cb-57a1e8134d8f"
      },
      "source": [
        "model = create_model(roberta_model, max_len)\n",
        "model.summary()"
      ],
      "id": "pointed-repeat",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 48)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 48)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_1[0][0]',                \n",
            " el)                            thPoolingAndCrossAt               'input_2[0][0]']                \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 48,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  [(None, 48, 200),    695200      ['tf_roberta_model[0][0]']       \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100),                                                     \n",
            "                                 (None, 100)]                                                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 200)          0           ['bidirectional[0][1]',          \n",
            "                                                                  'bidirectional[0][3]']          \n",
            "                                                                                                  \n",
            " attention (Attention)          ((None, 200),        12091       ['bidirectional[0][0]',          \n",
            "                                 (None, 48, 1))                   'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 28)           5628        ['attention[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 125,358,551\n",
            "Trainable params: 125,358,551\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monetary-belgium"
      },
      "source": [
        "# Model Training"
      ],
      "id": "monetary-belgium"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58o5gjJBvOYg"
      },
      "source": [
        "chechpoint = keras.callbacks.ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Weights/GoEmo_Roberta+BiLSTM+Attention+Dense_weights.best.h5',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")"
      ],
      "id": "58o5gjJBvOYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------"
      ],
      "metadata": {
        "id": "KTt-_4oe680Q"
      },
      "id": "KTt-_4oe680Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roberta + Dense**"
      ],
      "metadata": {
        "id": "6Diy59s7WL3Y"
      },
      "id": "6Diy59s7WL3Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-3gDEe0hrcv"
      },
      "source": [
        "# Roberta + Dense\n",
        "history = model.fit([train_input_ids,train_attention_masks],\n",
        "                    y_train,\n",
        "                    validation_data=([val_input_ids,val_attention_masks],y_val),\n",
        "                    verbose = 1,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    callbacks=[chechpoint]\n",
        "                    )"
      ],
      "id": "_-3gDEe0hrcv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/Weights/GoEmo_Roberta+Dense_weights.best.h5')\n",
        "\n",
        "y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "# Model evaluation function\n",
        "def model_eval(y_true, y_pred_labels, emotions):\n",
        "\n",
        "    # Defining variables\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "    # Per emotion evaluation\n",
        "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
        "\n",
        "    for i in range(len(emotions)):\n",
        "\n",
        "        # Computing precision, recall and f1-score\n",
        "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
        "\n",
        "        # Append results in lists\n",
        "        precision.append(round(p, 2))\n",
        "        recall.append(round(r, 2))\n",
        "        f1.append(round(f1_score, 2))\n",
        "\n",
        "    # Macro evaluation\n",
        "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
        "    # Append results in lists\n",
        "    precision.append(round(macro_p, 2))\n",
        "    recall.append(round(macro_r, 2))\n",
        "    f1.append(round(macro_f1_score, 2))\n",
        "\n",
        "    precision.append('-')\n",
        "    recall.append('-')\n",
        "    f1.append('-')\n",
        "\n",
        "    precision.append(round(macro_p, 4))\n",
        "    recall.append(round(macro_r, 4))\n",
        "    f1.append(round(macro_f1_score, 4))\n",
        "\n",
        "    # Micro evaluation\n",
        "    micro_p, micro_r, micro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"micro\")\n",
        "    # Append results in lists\n",
        "    precision.append(round(micro_p, 4))\n",
        "    recall.append(round(micro_r, 4))\n",
        "    f1.append(round(micro_f1_score, 4))\n",
        "\n",
        "\n",
        "\n",
        "    # jaccard_score evaluation    [None, 'micro', 'macro', 'weighted', 'samples']\n",
        "    jaccard_micro = jaccard_score(y_true, y_pred_labels, average='micro')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_micro, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_macro = jaccard_score(y_true, y_pred_labels, average='macro')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_macro, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_weighted = jaccard_score(y_true, y_pred_labels, average='weighted')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_weighted, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_samples = jaccard_score(y_true, y_pred_labels, average='samples')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_samples, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # hamming_loss evaluation\n",
        "    HL = hamming_loss(y_true, y_pred_labels)\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(HL, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "\n",
        "    # Converting results to a dataframe\n",
        "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
        "    df_results.index = emotions+['MACRO-AVERAGE']+['-']+['MACRO-AVERAGE']+['MICRO-AVERAGE']+['jaccard_micro']+['jaccard_macro']+['jaccard_weighted']+['jaccard_samples']+['HL']\n",
        "\n",
        "    return df_results\n",
        "\n",
        "\n",
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "def proba_to_labels_difference_from_mean(y_pred_proba):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        mean = statistics.mean(y_pred_proba[i])\n",
        "        difference_from_mean = np.zeros(y_pred_proba.shape[1])\n",
        "        for x in range(y_pred_proba.shape[1]):\n",
        "            difference_from_mean[x] = (y_pred_proba[i][x])**2 - mean**2\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if difference_from_mean[j] > mean*2:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels_difference_from_mean(y_pred_proba)\n",
        "\n",
        "# Model evaluation\n",
        "# model_eval(y_test, y_pred_labels, Lable_names)\n",
        "\n",
        "\n",
        "# sum(np.sum(y_pred_labels, axis=1)==0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_test_n = np.copy(y_pred_labels)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_test_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_test_n, Lable_names)\n",
        "# sum(np.sum(y_pred_labels_test_n, axis=1)==0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "biJP8N9c6cRs",
        "outputId": "213d1a10-44d5-4e05-a56b-456edcba2152"
      },
      "id": "biJP8N9c6cRs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-51a0e296-7ab1-4312-bbff-1b472df6de48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.7</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.39</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.41</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.28</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.4427</td>\n",
              "      <td>0.5599</td>\n",
              "      <td>0.4819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO-AVERAGE</th>\n",
              "      <td>0.5264</td>\n",
              "      <td>0.5296</td>\n",
              "      <td>0.528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_micro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3587</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_macro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_weighted</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3793</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_samples</th>\n",
              "      <td>-</td>\n",
              "      <td>0.5032</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HL</th>\n",
              "      <td>-</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51a0e296-7ab1-4312-bbff-1b472df6de48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51a0e296-7ab1-4312-bbff-1b472df6de48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51a0e296-7ab1-4312-bbff-1b472df6de48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Precision  Recall      F1\n",
              "admiration             0.7    0.61    0.65\n",
              "amusement             0.73     0.9    0.81\n",
              "anger                 0.38    0.54    0.45\n",
              "annoyance             0.34     0.3    0.32\n",
              "approval              0.39     0.4    0.39\n",
              "caring                0.31    0.61    0.41\n",
              "confusion             0.32    0.49    0.39\n",
              "curiosity             0.47    0.69    0.56\n",
              "desire                0.46    0.58    0.51\n",
              "disappointment        0.34    0.31    0.33\n",
              "disapproval           0.34     0.5    0.41\n",
              "disgust               0.41    0.56    0.47\n",
              "embarrassment         0.29    0.49    0.36\n",
              "excitement             0.3    0.52    0.38\n",
              "fear                  0.55    0.78    0.65\n",
              "gratitude             0.92    0.87    0.89\n",
              "grief                 0.25     0.5    0.33\n",
              "joy                   0.54     0.6    0.57\n",
              "love                  0.73    0.85    0.79\n",
              "nervousness           0.28    0.52    0.36\n",
              "optimism              0.48    0.42    0.45\n",
              "pride                 0.22     0.5    0.31\n",
              "realization           0.19    0.22    0.21\n",
              "relief                0.21    0.55     0.3\n",
              "remorse               0.52    0.82    0.63\n",
              "sadness               0.48    0.54    0.51\n",
              "surprise              0.46     0.6    0.52\n",
              "neutral               0.78     0.4    0.53\n",
              "MACRO-AVERAGE         0.44    0.56    0.48\n",
              "-                        -       -       -\n",
              "MACRO-AVERAGE       0.4427  0.5599  0.4819\n",
              "MICRO-AVERAGE       0.5264  0.5296   0.528\n",
              "jaccard_micro            -  0.3587       -\n",
              "jaccard_macro            -  0.3345       -\n",
              "jaccard_weighted         -  0.3793       -\n",
              "jaccard_samples          -  0.5032       -\n",
              "HL                       -  0.0394       -"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roberta + BiLSTM + Attention + Dense"
      ],
      "metadata": {
        "id": "w7_MJWR4ozfX"
      },
      "id": "w7_MJWR4ozfX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Roberta + BiLSTM + Attention + Dense\n",
        "history = model.fit([train_input_ids,train_attention_masks],\n",
        "                    y_train,\n",
        "                    validation_data=([val_input_ids,val_attention_masks],y_val),\n",
        "                    verbose = 1,\n",
        "                    epochs=10,\n",
        "                    batch_size=64,\n",
        "                    callbacks=[chechpoint]\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MOx59PloyFL",
        "outputId": "cf89bef1-04a9-4f9b-f1f8-ed7157cfe3a9"
      },
      "id": "6MOx59PloyFL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "679/679 [==============================] - 1127s 2s/step - loss: 0.4970 - accuracy: 0.3159 - val_loss: 0.3758 - val_accuracy: 0.4411\n",
            "Epoch 2/10\n",
            "679/679 [==============================] - 1105s 2s/step - loss: 0.3557 - accuracy: 0.4402 - val_loss: 0.3402 - val_accuracy: 0.4547\n",
            "Epoch 3/10\n",
            "679/679 [==============================] - 1099s 2s/step - loss: 0.3085 - accuracy: 0.4710 - val_loss: 0.3217 - val_accuracy: 0.4573\n",
            "Epoch 4/10\n",
            "679/679 [==============================] - 1092s 2s/step - loss: 0.2796 - accuracy: 0.4843 - val_loss: 0.3178 - val_accuracy: 0.4693\n",
            "Epoch 5/10\n",
            "679/679 [==============================] - 1102s 2s/step - loss: 0.2559 - accuracy: 0.5006 - val_loss: 0.3165 - val_accuracy: 0.4774\n",
            "Epoch 6/10\n",
            "679/679 [==============================] - 1104s 2s/step - loss: 0.2362 - accuracy: 0.5147 - val_loss: 0.3159 - val_accuracy: 0.4953\n",
            "Epoch 7/10\n",
            "679/679 [==============================] - 1106s 2s/step - loss: 0.2192 - accuracy: 0.5266 - val_loss: 0.3188 - val_accuracy: 0.4796\n",
            "Epoch 8/10\n",
            "679/679 [==============================] - 1098s 2s/step - loss: 0.2087 - accuracy: 0.5396 - val_loss: 0.3236 - val_accuracy: 0.4712\n",
            "Epoch 9/10\n",
            "679/679 [==============================] - 1096s 2s/step - loss: 0.1939 - accuracy: 0.5502 - val_loss: 0.3339 - val_accuracy: 0.4905\n",
            "Epoch 10/10\n",
            "584/679 [========================>.....] - ETA: 2:28 - loss: 0.1823 - accuracy: 0.5647"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/Colab Notebooks/Weights/GoEmo_Roberta+BiLSTM+Attention+Dense_weights.best.h5')\n",
        "\n",
        "y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "# Model evaluation function\n",
        "def model_eval(y_true, y_pred_labels, emotions):\n",
        "\n",
        "    # Defining variables\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "    # Per emotion evaluation\n",
        "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
        "\n",
        "    for i in range(len(emotions)):\n",
        "\n",
        "        # Computing precision, recall and f1-score\n",
        "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
        "\n",
        "        # Append results in lists\n",
        "        precision.append(round(p, 2))\n",
        "        recall.append(round(r, 2))\n",
        "        f1.append(round(f1_score, 2))\n",
        "\n",
        "    # Macro evaluation\n",
        "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
        "    # Append results in lists\n",
        "    precision.append(round(macro_p, 2))\n",
        "    recall.append(round(macro_r, 2))\n",
        "    f1.append(round(macro_f1_score, 2))\n",
        "\n",
        "    precision.append('-')\n",
        "    recall.append('-')\n",
        "    f1.append('-')\n",
        "\n",
        "    precision.append(round(macro_p, 4))\n",
        "    recall.append(round(macro_r, 4))\n",
        "    f1.append(round(macro_f1_score, 4))\n",
        "\n",
        "    # Micro evaluation\n",
        "    micro_p, micro_r, micro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"micro\")\n",
        "    # Append results in lists\n",
        "    precision.append(round(micro_p, 4))\n",
        "    recall.append(round(micro_r, 4))\n",
        "    f1.append(round(micro_f1_score, 4))\n",
        "\n",
        "\n",
        "\n",
        "    # jaccard_score evaluation    [None, 'micro', 'macro', 'weighted', 'samples']\n",
        "    jaccard_micro = jaccard_score(y_true, y_pred_labels, average='micro')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_micro, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_macro = jaccard_score(y_true, y_pred_labels, average='macro')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_macro, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_weighted = jaccard_score(y_true, y_pred_labels, average='weighted')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_weighted, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_samples = jaccard_score(y_true, y_pred_labels, average='samples')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_samples, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # hamming_loss evaluation\n",
        "    HL = hamming_loss(y_true, y_pred_labels)\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(HL, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "\n",
        "    # Converting results to a dataframe\n",
        "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
        "    df_results.index = emotions+['MACRO-AVERAGE']+['-']+['MACRO-AVERAGE']+['MICRO-AVERAGE']+['jaccard_micro']+['jaccard_macro']+['jaccard_weighted']+['jaccard_samples']+['HL']\n",
        "\n",
        "    return df_results\n",
        "\n",
        "\n",
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "def proba_to_labels_difference_from_mean(y_pred_proba):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        mean = statistics.mean(y_pred_proba[i])\n",
        "        difference_from_mean = np.zeros(y_pred_proba.shape[1])\n",
        "        for x in range(y_pred_proba.shape[1]):\n",
        "            difference_from_mean[x] = (y_pred_proba[i][x])**2 - mean**2\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if difference_from_mean[j] > mean*2:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels_difference_from_mean(y_pred_proba)\n",
        "\n",
        "# Model evaluation\n",
        "# model_eval(y_test, y_pred_labels, Lable_names)\n",
        "\n",
        "\n",
        "# sum(np.sum(y_pred_labels, axis=1)==0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_test_n = np.copy(y_pred_labels)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_test_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_test_n, Lable_names)\n",
        "# sum(np.sum(y_pred_labels_test_n, axis=1)==0)\n"
      ],
      "metadata": {
        "id": "JHPdfGT06dJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95d0d38c-a623-4342-9800-d365d308a855"
      },
      "id": "JHPdfGT06dJu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b08d0e6b-83ed-47b2-9f2a-2e0422f9f3cc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.69</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.4512</td>\n",
              "      <td>0.5579</td>\n",
              "      <td>0.4803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO-AVERAGE</th>\n",
              "      <td>0.5364</td>\n",
              "      <td>0.5148</td>\n",
              "      <td>0.5254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_micro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3563</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_macro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3341</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_weighted</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3707</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_samples</th>\n",
              "      <td>-</td>\n",
              "      <td>0.5026</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HL</th>\n",
              "      <td>-</td>\n",
              "      <td>0.0387</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b08d0e6b-83ed-47b2-9f2a-2e0422f9f3cc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b08d0e6b-83ed-47b2-9f2a-2e0422f9f3cc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b08d0e6b-83ed-47b2-9f2a-2e0422f9f3cc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 Precision  Recall      F1\n",
              "admiration            0.69    0.63    0.66\n",
              "amusement             0.75    0.89    0.82\n",
              "anger                 0.38    0.53    0.45\n",
              "annoyance             0.38    0.24     0.3\n",
              "approval               0.4    0.41     0.4\n",
              "caring                0.34    0.51    0.41\n",
              "confusion             0.34    0.52    0.41\n",
              "curiosity             0.47    0.69    0.56\n",
              "desire                0.46    0.51    0.48\n",
              "disappointment        0.34     0.3    0.32\n",
              "disapproval           0.35    0.53    0.42\n",
              "disgust                0.4    0.64    0.49\n",
              "embarrassment         0.37    0.46    0.41\n",
              "excitement            0.42    0.47    0.44\n",
              "fear                   0.5    0.76    0.61\n",
              "gratitude             0.92    0.89     0.9\n",
              "grief                 0.16     0.5    0.24\n",
              "joy                   0.57     0.5    0.54\n",
              "love                  0.73    0.83    0.78\n",
              "nervousness           0.23    0.48    0.31\n",
              "optimism              0.51    0.53    0.52\n",
              "pride                 0.24    0.56    0.34\n",
              "realization           0.25    0.17     0.2\n",
              "relief                0.16    0.73    0.26\n",
              "remorse               0.55    0.82    0.66\n",
              "sadness               0.54    0.51    0.52\n",
              "surprise              0.42    0.66    0.52\n",
              "neutral               0.74    0.36    0.48\n",
              "MACRO-AVERAGE         0.45    0.56    0.48\n",
              "-                        -       -       -\n",
              "MACRO-AVERAGE       0.4512  0.5579  0.4803\n",
              "MICRO-AVERAGE       0.5364  0.5148  0.5254\n",
              "jaccard_micro            -  0.3563       -\n",
              "jaccard_macro            -  0.3341       -\n",
              "jaccard_weighted         -  0.3707       -\n",
              "jaccard_samples          -  0.5026       -\n",
              "HL                       -  0.0387       -"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSZeqN8-6dNN"
      },
      "id": "bSZeqN8-6dNN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUZfy3G26BSt"
      },
      "id": "kUZfy3G26BSt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-bandwidth"
      },
      "source": [
        "**Plotting Accuracy and Loss (Training and Validation)**"
      ],
      "id": "arabic-bandwidth"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "domestic-consultancy",
        "outputId": "bb9b45bd-1840-46b9-9a6d-81ac1c7425f1"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "domestic-consultancy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 10\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m1xe8s1sjtgN"
      },
      "id": "m1xe8s1sjtgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "continental-leave",
        "outputId": "ff010964-7f4c-48a5-a81c-55afea1c43bd"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "continental-leave",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 10\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zroP__p4kKNn"
      },
      "id": "zroP__p4kKNn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "japanese-interview"
      },
      "source": [
        "**RoBERTa Model prediction on Test Data**"
      ],
      "id": "japanese-interview"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdeQ_laxJ_fx"
      },
      "source": [
        "y_pred_proba = model.predict([test_input_ids,test_attention_masks])"
      ],
      "id": "OdeQ_laxJ_fx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13c710aa"
      },
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "# Model evaluation function\n",
        "def model_eval(y_true, y_pred_labels, emotions):\n",
        "\n",
        "    # Defining variables\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "    # Per emotion evaluation\n",
        "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
        "\n",
        "    for i in range(len(emotions)):\n",
        "\n",
        "        # Computing precision, recall and f1-score\n",
        "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
        "\n",
        "        # Append results in lists\n",
        "        precision.append(round(p, 2))\n",
        "        recall.append(round(r, 2))\n",
        "        f1.append(round(f1_score, 2))\n",
        "\n",
        "    # Macro evaluation\n",
        "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
        "    # Append results in lists\n",
        "    precision.append(round(macro_p, 2))\n",
        "    recall.append(round(macro_r, 2))\n",
        "    f1.append(round(macro_f1_score, 2))\n",
        "\n",
        "    precision.append('-')\n",
        "    recall.append('-')\n",
        "    f1.append('-')\n",
        "\n",
        "    precision.append(round(macro_p, 4))\n",
        "    recall.append(round(macro_r, 4))\n",
        "    f1.append(round(macro_f1_score, 4))\n",
        "\n",
        "    # Micro evaluation\n",
        "    micro_p, micro_r, micro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"micro\")\n",
        "    # Append results in lists\n",
        "    precision.append(round(micro_p, 4))\n",
        "    recall.append(round(micro_r, 4))\n",
        "    f1.append(round(micro_f1_score, 4))\n",
        "\n",
        "\n",
        "\n",
        "    # jaccard_score evaluation    [None, 'micro', 'macro', 'weighted', 'samples']\n",
        "    jaccard_micro = jaccard_score(y_true, y_pred_labels, average='micro')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_micro, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_macro = jaccard_score(y_true, y_pred_labels, average='macro')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_macro, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_weighted = jaccard_score(y_true, y_pred_labels, average='weighted')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_weighted, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "    jaccard_samples = jaccard_score(y_true, y_pred_labels, average='samples')\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(jaccard_samples, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # hamming_loss evaluation\n",
        "    HL = hamming_loss(y_true, y_pred_labels)\n",
        "    # Append results in lists\n",
        "    precision.append('-')\n",
        "    recall.append(round(HL, 4))\n",
        "    f1.append('-')\n",
        "\n",
        "\n",
        "    # Converting results to a dataframe\n",
        "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
        "    df_results.index = emotions+['MACRO-AVERAGE']+['-']+['MACRO-AVERAGE']+['MICRO-AVERAGE']+['jaccard_micro']+['jaccard_macro']+['jaccard_weighted']+['jaccard_samples']+['HL']\n",
        "\n",
        "    return df_results\n"
      ],
      "id": "13c710aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AIqxG7xiqLi9",
        "outputId": "69dd20c1-acdd-4aff-8c38-8f186f80dca0"
      },
      "source": [
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "def proba_to_labels_difference_from_mean(y_pred_proba):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        mean = statistics.mean(y_pred_proba[i])\n",
        "        difference_from_mean = np.zeros(y_pred_proba.shape[1])\n",
        "        for x in range(y_pred_proba.shape[1]):\n",
        "            difference_from_mean[x] = (y_pred_proba[i][x])**2 - mean**2\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if difference_from_mean[j] > mean*2:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels_difference_from_mean(y_pred_proba)\n",
        "\n",
        "# Model evaluation\n",
        "# model_eval(y_test, y_pred_labels, Lable_names)\n",
        "\n",
        "\n",
        "# sum(np.sum(y_pred_labels, axis=1)==0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_test_n = np.copy(y_pred_labels)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_test_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_test_n, Lable_names)\n",
        "# sum(np.sum(y_pred_labels_test_n, axis=1)==0)"
      ],
      "id": "AIqxG7xiqLi9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.4451</td>\n",
              "      <td>0.5527</td>\n",
              "      <td>0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO-AVERAGE</th>\n",
              "      <td>0.5339</td>\n",
              "      <td>0.5089</td>\n",
              "      <td>0.5211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_micro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3524</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_macro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.327</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_weighted</th>\n",
              "      <td>-</td>\n",
              "      <td>0.369</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_samples</th>\n",
              "      <td>-</td>\n",
              "      <td>0.5024</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HL</th>\n",
              "      <td>-</td>\n",
              "      <td>0.039</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Precision  Recall      F1\n",
              "admiration            0.67    0.61    0.64\n",
              "amusement             0.76    0.89    0.82\n",
              "anger                 0.36    0.57    0.44\n",
              "annoyance             0.37    0.18    0.25\n",
              "approval              0.46    0.32    0.38\n",
              "caring                0.31    0.63    0.42\n",
              "confusion             0.35     0.5    0.41\n",
              "curiosity             0.49    0.69    0.57\n",
              "desire                 0.5    0.51     0.5\n",
              "disappointment        0.31    0.26    0.28\n",
              "disapproval           0.35    0.47     0.4\n",
              "disgust                0.4    0.66     0.5\n",
              "embarrassment          0.3    0.49    0.37\n",
              "excitement            0.35     0.5    0.41\n",
              "fear                  0.53    0.76    0.62\n",
              "gratitude             0.91    0.88     0.9\n",
              "grief                 0.14    0.83    0.24\n",
              "joy                   0.58     0.5    0.54\n",
              "love                  0.75    0.82    0.78\n",
              "nervousness           0.17    0.48    0.26\n",
              "optimism              0.56    0.46     0.5\n",
              "pride                 0.25    0.38     0.3\n",
              "realization            0.2    0.21    0.21\n",
              "relief                0.17    0.55    0.26\n",
              "remorse               0.53    0.84    0.65\n",
              "sadness               0.42    0.53    0.47\n",
              "surprise              0.51     0.6    0.55\n",
              "neutral               0.74    0.38     0.5\n",
              "MACRO-AVERAGE         0.45    0.55    0.47\n",
              "-                        -       -       -\n",
              "MACRO-AVERAGE       0.4451  0.5527  0.4704\n",
              "MICRO-AVERAGE       0.5339  0.5089  0.5211\n",
              "jaccard_micro            -  0.3524       -\n",
              "jaccard_macro            -   0.327       -\n",
              "jaccard_weighted         -   0.369       -\n",
              "jaccard_samples          -  0.5024       -\n",
              "HL                       -   0.039       -"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rb75op7a_4k7",
        "outputId": "4a6b574d-29e9-40c7-9d26-8dc54be2b89f"
      },
      "source": [
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "def proba_to_labels_difference_from_mean(y_pred_proba):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        mean = statistics.mean(y_pred_proba[i])\n",
        "        difference_from_mean = np.zeros(y_pred_proba.shape[1])\n",
        "        for x in range(y_pred_proba.shape[1]):\n",
        "            difference_from_mean[x] = (y_pred_proba[i][x])**2 - mean**2\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if difference_from_mean[j] > mean*2:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels_difference_from_mean(y_pred_proba)\n",
        "\n",
        "\n",
        "\n",
        "# Model evaluation\n",
        "# model_eval(y_test, y_pred_labels, Lable_names)\n",
        "\n",
        "\n",
        "# sum(np.sum(y_pred_labels, axis=1)==0)\n",
        "\n",
        "\n",
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_test_n = np.copy(y_pred_labels)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_test_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_test_n, Lable_names)\n",
        "# sum(np.sum(y_pred_labels_test_n, axis=1)==0)"
      ],
      "id": "rb75op7a_4k7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.4451</td>\n",
              "      <td>0.5527</td>\n",
              "      <td>0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO-AVERAGE</th>\n",
              "      <td>0.5339</td>\n",
              "      <td>0.5089</td>\n",
              "      <td>0.5211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_micro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3524</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_macro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.327</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_weighted</th>\n",
              "      <td>-</td>\n",
              "      <td>0.369</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_samples</th>\n",
              "      <td>-</td>\n",
              "      <td>0.5024</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HL</th>\n",
              "      <td>-</td>\n",
              "      <td>0.039</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Precision  Recall      F1\n",
              "admiration            0.67    0.61    0.64\n",
              "amusement             0.76    0.89    0.82\n",
              "anger                 0.36    0.57    0.44\n",
              "annoyance             0.37    0.18    0.25\n",
              "approval              0.46    0.32    0.38\n",
              "caring                0.31    0.63    0.42\n",
              "confusion             0.35     0.5    0.41\n",
              "curiosity             0.49    0.69    0.57\n",
              "desire                 0.5    0.51     0.5\n",
              "disappointment        0.31    0.26    0.28\n",
              "disapproval           0.35    0.47     0.4\n",
              "disgust                0.4    0.66     0.5\n",
              "embarrassment          0.3    0.49    0.37\n",
              "excitement            0.35     0.5    0.41\n",
              "fear                  0.53    0.76    0.62\n",
              "gratitude             0.91    0.88     0.9\n",
              "grief                 0.14    0.83    0.24\n",
              "joy                   0.58     0.5    0.54\n",
              "love                  0.75    0.82    0.78\n",
              "nervousness           0.17    0.48    0.26\n",
              "optimism              0.56    0.46     0.5\n",
              "pride                 0.25    0.38     0.3\n",
              "realization            0.2    0.21    0.21\n",
              "relief                0.17    0.55    0.26\n",
              "remorse               0.53    0.84    0.65\n",
              "sadness               0.42    0.53    0.47\n",
              "surprise              0.51     0.6    0.55\n",
              "neutral               0.74    0.38     0.5\n",
              "MACRO-AVERAGE         0.45    0.55    0.47\n",
              "-                        -       -       -\n",
              "MACRO-AVERAGE       0.4451  0.5527  0.4704\n",
              "MICRO-AVERAGE       0.5339  0.5089  0.5211\n",
              "jaccard_micro            -  0.3524       -\n",
              "jaccard_macro            -   0.327       -\n",
              "jaccard_weighted         -   0.369       -\n",
              "jaccard_samples          -  0.5024       -\n",
              "HL                       -   0.039       -"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SpIbSyakSU1b",
        "outputId": "12086297-9f76-4712-84c7-2abd4a97a51a"
      },
      "source": [
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "def proba_to_labels_difference_from_mean(y_pred_proba):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        mean = statistics.mean(y_pred_proba[i])\n",
        "        difference_from_mean = np.zeros(y_pred_proba.shape[1])\n",
        "        for x in range(y_pred_proba.shape[1]):\n",
        "            difference_from_mean[x] = (y_pred_proba[i][x])**2 - mean**2\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if difference_from_mean[j] > mean*2:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels_difference_from_mean(y_pred_proba)\n",
        "\n",
        "# Model evaluation\n",
        "# model_eval(y_test, y_pred_labels, Lable_names)\n",
        "\n",
        "\n",
        "# sum(np.sum(y_pred_labels, axis=1)==0)\n",
        "\n",
        "\n",
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_test_n = np.copy(y_pred_labels)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_test_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_test_n, Lable_names)\n",
        "# sum(np.sum(y_pred_labels_test_n, axis=1)==0)"
      ],
      "id": "SpIbSyakSU1b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.4451</td>\n",
              "      <td>0.5527</td>\n",
              "      <td>0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO-AVERAGE</th>\n",
              "      <td>0.5339</td>\n",
              "      <td>0.5089</td>\n",
              "      <td>0.5211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_micro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3524</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_macro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.327</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_weighted</th>\n",
              "      <td>-</td>\n",
              "      <td>0.369</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_samples</th>\n",
              "      <td>-</td>\n",
              "      <td>0.5024</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HL</th>\n",
              "      <td>-</td>\n",
              "      <td>0.039</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Precision  Recall      F1\n",
              "admiration            0.67    0.61    0.64\n",
              "amusement             0.76    0.89    0.82\n",
              "anger                 0.36    0.57    0.44\n",
              "annoyance             0.37    0.18    0.25\n",
              "approval              0.46    0.32    0.38\n",
              "caring                0.31    0.63    0.42\n",
              "confusion             0.35     0.5    0.41\n",
              "curiosity             0.49    0.69    0.57\n",
              "desire                 0.5    0.51     0.5\n",
              "disappointment        0.31    0.26    0.28\n",
              "disapproval           0.35    0.47     0.4\n",
              "disgust                0.4    0.66     0.5\n",
              "embarrassment          0.3    0.49    0.37\n",
              "excitement            0.35     0.5    0.41\n",
              "fear                  0.53    0.76    0.62\n",
              "gratitude             0.91    0.88     0.9\n",
              "grief                 0.14    0.83    0.24\n",
              "joy                   0.58     0.5    0.54\n",
              "love                  0.75    0.82    0.78\n",
              "nervousness           0.17    0.48    0.26\n",
              "optimism              0.56    0.46     0.5\n",
              "pride                 0.25    0.38     0.3\n",
              "realization            0.2    0.21    0.21\n",
              "relief                0.17    0.55    0.26\n",
              "remorse               0.53    0.84    0.65\n",
              "sadness               0.42    0.53    0.47\n",
              "surprise              0.51     0.6    0.55\n",
              "neutral               0.74    0.38     0.5\n",
              "MACRO-AVERAGE         0.45    0.55    0.47\n",
              "-                        -       -       -\n",
              "MACRO-AVERAGE       0.4451  0.5527  0.4704\n",
              "MICRO-AVERAGE       0.5339  0.5089  0.5211\n",
              "jaccard_micro            -  0.3524       -\n",
              "jaccard_macro            -   0.327       -\n",
              "jaccard_weighted         -   0.369       -\n",
              "jaccard_samples          -  0.5024       -\n",
              "HL                       -   0.039       -"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f0UYUybygqK1",
        "outputId": "ebb2a71b-bffb-4355-c093-5e6df11b8d88"
      },
      "source": [
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "def proba_to_labels_difference_from_mean(y_pred_proba):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        mean = statistics.mean(y_pred_proba[i])\n",
        "        difference_from_mean = np.zeros(y_pred_proba.shape[1])\n",
        "        for x in range(y_pred_proba.shape[1]):\n",
        "            difference_from_mean[x] = (y_pred_proba[i][x])**2 - mean**2\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if difference_from_mean[j] > mean*2:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels_difference_from_mean(y_pred_proba)\n",
        "\n",
        "# Model evaluation\n",
        "# model_eval(y_test, y_pred_labels, Lable_names)\n",
        "\n",
        "\n",
        "# sum(np.sum(y_pred_labels, axis=1)==0)\n",
        "\n",
        "\n",
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_test_n = np.copy(y_pred_labels)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_test_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_test_n, Lable_names)\n",
        "# sum(np.sum(y_pred_labels_test_n, axis=1)==0)"
      ],
      "id": "f0UYUybygqK1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.49</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.4</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.74</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-</th>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.4451</td>\n",
              "      <td>0.5527</td>\n",
              "      <td>0.4704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO-AVERAGE</th>\n",
              "      <td>0.5339</td>\n",
              "      <td>0.5089</td>\n",
              "      <td>0.5211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_micro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.3524</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_macro</th>\n",
              "      <td>-</td>\n",
              "      <td>0.327</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_weighted</th>\n",
              "      <td>-</td>\n",
              "      <td>0.369</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jaccard_samples</th>\n",
              "      <td>-</td>\n",
              "      <td>0.5024</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HL</th>\n",
              "      <td>-</td>\n",
              "      <td>0.039</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Precision  Recall      F1\n",
              "admiration            0.67    0.61    0.64\n",
              "amusement             0.76    0.89    0.82\n",
              "anger                 0.36    0.57    0.44\n",
              "annoyance             0.37    0.18    0.25\n",
              "approval              0.46    0.32    0.38\n",
              "caring                0.31    0.63    0.42\n",
              "confusion             0.35     0.5    0.41\n",
              "curiosity             0.49    0.69    0.57\n",
              "desire                 0.5    0.51     0.5\n",
              "disappointment        0.31    0.26    0.28\n",
              "disapproval           0.35    0.47     0.4\n",
              "disgust                0.4    0.66     0.5\n",
              "embarrassment          0.3    0.49    0.37\n",
              "excitement            0.35     0.5    0.41\n",
              "fear                  0.53    0.76    0.62\n",
              "gratitude             0.91    0.88     0.9\n",
              "grief                 0.14    0.83    0.24\n",
              "joy                   0.58     0.5    0.54\n",
              "love                  0.75    0.82    0.78\n",
              "nervousness           0.17    0.48    0.26\n",
              "optimism              0.56    0.46     0.5\n",
              "pride                 0.25    0.38     0.3\n",
              "realization            0.2    0.21    0.21\n",
              "relief                0.17    0.55    0.26\n",
              "remorse               0.53    0.84    0.65\n",
              "sadness               0.42    0.53    0.47\n",
              "surprise              0.51     0.6    0.55\n",
              "neutral               0.74    0.38     0.5\n",
              "MACRO-AVERAGE         0.45    0.55    0.47\n",
              "-                        -       -       -\n",
              "MACRO-AVERAGE       0.4451  0.5527  0.4704\n",
              "MICRO-AVERAGE       0.5339  0.5089  0.5211\n",
              "jaccard_micro            -  0.3524       -\n",
              "jaccard_macro            -   0.327       -\n",
              "jaccard_weighted         -   0.369       -\n",
              "jaccard_samples          -  0.5024       -\n",
              "HL                       -   0.039       -"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mQTnrhCqJa6"
      },
      "source": [
        "-------------"
      ],
      "id": "1mQTnrhCqJa6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uvx8K0XZrNpS",
        "outputId": "76e71a91-8040-4cbe-ea14-1c06292e0c40"
      },
      "source": [
        "# y_pred_proba = model.predict([test_input_ids,test_attention_masks])\n",
        "\n",
        "# from probabilities to labels using a given threshold\n",
        "def proba_to_labels(y_pred_proba, threshold=0.5):\n",
        "\n",
        "    y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "\n",
        "    for i in range(y_pred_proba.shape[0]):\n",
        "        for j in range(y_pred_proba.shape[1]):\n",
        "            if y_pred_proba[i][j] > threshold:\n",
        "                y_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                y_pred_labels[i][j] = 0\n",
        "\n",
        "    return y_pred_labels\n",
        "\n",
        "# Generate labels\n",
        "y_pred_labels = proba_to_labels(y_pred_proba)\n",
        "\n",
        "# Model evaluation function\n",
        "def model_eval(y_true, y_pred_labels, emotions):\n",
        "\n",
        "    # Defining variables\n",
        "    precision = []\n",
        "    recall = []\n",
        "    f1 = []\n",
        "\n",
        "    # Per emotion evaluation\n",
        "    idx2emotion = {i: e for i, e in enumerate(emotions)}\n",
        "\n",
        "    for i in range(len(emotions)):\n",
        "\n",
        "        # Computing precision, recall and f1-score\n",
        "        p, r, f1_score, _ = precision_recall_fscore_support(y_true[:, i], y_pred_labels[:, i], average=\"binary\")\n",
        "\n",
        "        # Append results in lists\n",
        "        precision.append(round(p, 2))\n",
        "        recall.append(round(r, 2))\n",
        "        f1.append(round(f1_score, 2))\n",
        "\n",
        "    # Macro evaluation\n",
        "    macro_p, macro_r, macro_f1_score, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
        "\n",
        "    # Append results in lists\n",
        "    precision.append(round(macro_p, 2))\n",
        "    recall.append(round(macro_r, 2))\n",
        "    f1.append(round(macro_f1_score, 2))\n",
        "\n",
        "    # Converting results to a dataframe\n",
        "    df_results = pd.DataFrame({\"Precision\":precision, \"Recall\":recall, 'F1':f1})\n",
        "    df_results.index = emotions+['MACRO-AVERAGE']\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# Model evaluation\n",
        "model_eval(y_test, y_pred_labels, Lable_names)"
      ],
      "id": "uvx8K0XZrNpS",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.58</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.62</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.59</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.66</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.95</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.27</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.43</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.71</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.29</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.16</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.57</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Precision  Recall    F1\n",
              "admiration           0.82    0.41  0.55\n",
              "amusement            0.80    0.77  0.78\n",
              "anger                0.54    0.39  0.46\n",
              "annoyance            0.00    0.00  0.00\n",
              "approval             0.73    0.17  0.28\n",
              "caring               0.44    0.39  0.41\n",
              "confusion            0.51    0.29  0.37\n",
              "curiosity            0.52    0.45  0.48\n",
              "desire               0.58    0.40  0.47\n",
              "disappointment       0.50    0.07  0.13\n",
              "disapproval          0.48    0.29  0.36\n",
              "disgust              0.55    0.42  0.48\n",
              "embarrassment        0.62    0.41  0.49\n",
              "excitement           0.59    0.29  0.39\n",
              "fear                 0.66    0.73  0.70\n",
              "gratitude            0.95    0.78  0.86\n",
              "grief                0.27    0.50  0.35\n",
              "joy                  0.81    0.27  0.41\n",
              "love                 0.82    0.68  0.74\n",
              "nervousness          0.43    0.39  0.41\n",
              "optimism             0.71    0.34  0.46\n",
              "pride                0.32    0.56  0.41\n",
              "realization          0.29    0.17  0.21\n",
              "relief               0.16    0.73  0.27\n",
              "remorse              0.56    0.79  0.66\n",
              "sadness              0.75    0.34  0.47\n",
              "surprise             0.55    0.52  0.53\n",
              "neutral              0.88    0.13  0.23\n",
              "MACRO-AVERAGE        0.57    0.42  0.44"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6BiebKNUfv4",
        "outputId": "b399b7f5-4bc1-4a76-e40b-12b759dee6e0"
      },
      "source": [
        "# Function that computes labels from probabilities and optimizes the threshold that maximizes f1-score\n",
        "def proba_to_labels_opt(y_true, y_pred_proba):\n",
        "\n",
        "    '''\n",
        "    Inputs:\n",
        "        y_true: Ground truth labels\n",
        "        y_pred_proba: predicted probabilities\n",
        "\n",
        "    Outputs :\n",
        "        best_y_pred_labels: preticted labels associated with best threshold\n",
        "        best_t: best threshold\n",
        "        best_macro_f1: macro f1-score associated with predicted labels\n",
        "    '''\n",
        "\n",
        "    # range of possible thresholds\n",
        "    thresholds = np.arange(0.1, 0.99, 0.01)\n",
        "\n",
        "    # Computing threshold that maximizes macro f1-score\n",
        "    best_y_pred_labels = np.zeros_like(y_pred_proba)\n",
        "    best_t = 0\n",
        "    best_macro_f1 = 0\n",
        "\n",
        "    # Iterating through possible thresholds\n",
        "    for t in thresholds:\n",
        "\n",
        "        y_pred_labels = proba_to_labels(y_pred_proba, t)\n",
        "\n",
        "        _, _, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred_labels, average=\"macro\")\n",
        "\n",
        "        if macro_f1 > best_macro_f1:\n",
        "            best_macro_f1 = macro_f1\n",
        "            best_t = t\n",
        "            best_y_pred_labels = y_pred_labels\n",
        "\n",
        "    return best_y_pred_labels, best_t, best_macro_f1\n",
        "\n",
        "# Compute label predictions and corresponding optimal thresholds\n",
        "y_pred_labels_opt, threshold_opt, macro_f1_opt = proba_to_labels_opt(y_test, y_pred_proba)\n",
        "print(\"The model's threshold is {}\".format(threshold_opt))\n",
        "print(\"The model's best macro-f1 is {}\".format(macro_f1_opt))"
      ],
      "id": "G6BiebKNUfv4",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model's threshold is 0.24999999999999992\n",
            "The model's best macro-f1 is 0.4713859409815534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "timnZxVvU4b_",
        "outputId": "b0e986e5-ac3e-4fdb-d31b-8b4cafb1ec0d"
      },
      "source": [
        "# Model evaluation\n",
        "model_eval(y_test, y_pred_labels_opt, Lable_names)"
      ],
      "id": "timnZxVvU4b_",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Precision  Recall    F1\n",
              "admiration           0.73    0.54  0.62\n",
              "amusement            0.77    0.89  0.82\n",
              "anger                0.34    0.57  0.43\n",
              "annoyance            0.40    0.25  0.31\n",
              "approval             0.47    0.29  0.36\n",
              "caring               0.33    0.53  0.41\n",
              "confusion            0.32    0.54  0.41\n",
              "curiosity            0.45    0.68  0.54\n",
              "desire               0.51    0.52  0.51\n",
              "disappointment       0.34    0.32  0.33\n",
              "disapproval          0.36    0.50  0.42\n",
              "disgust              0.40    0.62  0.49\n",
              "embarrassment        0.35    0.49  0.41\n",
              "excitement           0.38    0.47  0.42\n",
              "fear                 0.50    0.77  0.61\n",
              "gratitude            0.93    0.88  0.90\n",
              "grief                0.20    0.67  0.31\n",
              "joy                  0.55    0.48  0.51\n",
              "love                 0.75    0.80  0.77\n",
              "nervousness          0.24    0.52  0.33\n",
              "optimism             0.54    0.45  0.49\n",
              "pride                0.22    0.62  0.32\n",
              "realization          0.17    0.25  0.20\n",
              "relief               0.10    0.82  0.17\n",
              "remorse              0.51    0.86  0.64\n",
              "sadness              0.56    0.49  0.52\n",
              "surprise             0.44    0.62  0.52\n",
              "neutral              0.80    0.29  0.42\n",
              "MACRO-AVERAGE        0.45    0.56  0.47"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inxgzHBNVPN7",
        "outputId": "a138e1a7-3880-4fd2-ade5-0dd9f26f8892"
      },
      "source": [
        "# Number of predictions with no positive label\n",
        "sum(np.sum(y_pred_labels_opt, axis=1)==0)"
      ],
      "id": "inxgzHBNVPN7",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "350"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "pnCSAqf5VS2Y",
        "outputId": "bacf2d94-c25f-47ab-f1fc-37cd5fa40bda"
      },
      "source": [
        "# solution 1 (label with the highest probability)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_opt_h = np.copy(y_pred_labels_opt)\n",
        "\n",
        "# if no predictions ==> label with highest proba\n",
        "for i, pred in enumerate(y_pred_labels_opt_h):\n",
        "    if pred.sum()==0:\n",
        "        pred[np.argmax(y_pred_labels_opt[i])]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_opt_h, Lable_names)"
      ],
      "id": "pnCSAqf5VS2Y",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.41</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Precision  Recall    F1\n",
              "admiration           0.41    0.59  0.49\n",
              "amusement            0.77    0.89  0.82\n",
              "anger                0.34    0.57  0.43\n",
              "annoyance            0.40    0.25  0.31\n",
              "approval             0.47    0.29  0.36\n",
              "caring               0.33    0.53  0.41\n",
              "confusion            0.32    0.54  0.41\n",
              "curiosity            0.45    0.68  0.54\n",
              "desire               0.51    0.52  0.51\n",
              "disappointment       0.34    0.32  0.33\n",
              "disapproval          0.36    0.50  0.42\n",
              "disgust              0.40    0.62  0.49\n",
              "embarrassment        0.35    0.49  0.41\n",
              "excitement           0.38    0.47  0.42\n",
              "fear                 0.50    0.77  0.61\n",
              "gratitude            0.93    0.88  0.90\n",
              "grief                0.20    0.67  0.31\n",
              "joy                  0.55    0.48  0.51\n",
              "love                 0.75    0.80  0.77\n",
              "nervousness          0.24    0.52  0.33\n",
              "optimism             0.54    0.45  0.49\n",
              "pride                0.22    0.62  0.32\n",
              "realization          0.17    0.25  0.20\n",
              "relief               0.10    0.82  0.17\n",
              "remorse              0.51    0.86  0.64\n",
              "sadness              0.56    0.49  0.52\n",
              "surprise             0.44    0.62  0.52\n",
              "neutral              0.80    0.29  0.42\n",
              "MACRO-AVERAGE        0.44    0.56  0.47"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "bJiaDzRiVmWV",
        "outputId": "9ea2a422-e1bd-42d0-87de-1ba196e1be32"
      },
      "source": [
        "# solution 2 (assign the 'Neutral' emotion)\n",
        "# Handling empty predictions\n",
        "y_pred_labels_opt_n = np.copy(y_pred_labels_opt)\n",
        "\n",
        "# if no predictions ==> neutral\n",
        "for pred in y_pred_labels_opt_n:\n",
        "    if pred.sum()==0:\n",
        "        pred[-1]=1\n",
        "\n",
        "# Evaluation\n",
        "model_eval(y_test, y_pred_labels_opt_n, Lable_names)"
      ],
      "id": "bJiaDzRiVmWV",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>admiration</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amusement</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anger</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>annoyance</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>approval</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>caring</th>\n",
              "      <td>0.33</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confusion</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>curiosity</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desire</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disappointment</th>\n",
              "      <td>0.34</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disapproval</th>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarrassment</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>excitement</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gratitude</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grief</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joy</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>love</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nervousness</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>optimism</th>\n",
              "      <td>0.54</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>realization</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relief</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>remorse</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sadness</th>\n",
              "      <td>0.56</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MACRO-AVERAGE</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Precision  Recall    F1\n",
              "admiration           0.73    0.54  0.62\n",
              "amusement            0.77    0.89  0.82\n",
              "anger                0.34    0.57  0.43\n",
              "annoyance            0.40    0.25  0.31\n",
              "approval             0.47    0.29  0.36\n",
              "caring               0.33    0.53  0.41\n",
              "confusion            0.32    0.54  0.41\n",
              "curiosity            0.45    0.68  0.54\n",
              "desire               0.51    0.52  0.51\n",
              "disappointment       0.34    0.32  0.33\n",
              "disapproval          0.36    0.50  0.42\n",
              "disgust              0.40    0.62  0.49\n",
              "embarrassment        0.35    0.49  0.41\n",
              "excitement           0.38    0.47  0.42\n",
              "fear                 0.50    0.77  0.61\n",
              "gratitude            0.93    0.88  0.90\n",
              "grief                0.20    0.67  0.31\n",
              "joy                  0.55    0.48  0.51\n",
              "love                 0.75    0.80  0.77\n",
              "nervousness          0.24    0.52  0.33\n",
              "optimism             0.54    0.45  0.49\n",
              "pride                0.22    0.62  0.32\n",
              "realization          0.17    0.25  0.20\n",
              "relief               0.10    0.82  0.17\n",
              "remorse              0.51    0.86  0.64\n",
              "sadness              0.56    0.49  0.52\n",
              "surprise             0.44    0.62  0.52\n",
              "neutral              0.68    0.38  0.49\n",
              "MACRO-AVERAGE        0.45    0.56  0.47"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA_TaNX8ck8Z"
      },
      "source": [
        "result = model.predict([test_input_ids,test_attention_masks])\n",
        "y_pred = np.zeros_like(result)\n",
        "y_pred[np.arange(len(result)), result.argmax(1)] = 1"
      ],
      "id": "QA_TaNX8ck8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "residential-surgery"
      },
      "source": [
        "**Accuracy and F1 Score of Model**"
      ],
      "id": "residential-surgery"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adolescent-stock",
        "outputId": "872885cb-ed72-410f-e194-770b5e71c1d2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "adolescent-stock",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.39804914650159445\n",
            "F1 Score : 0.44020023217880555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28IqhHKgITZm",
        "outputId": "2950c24f-2f15-431b-85d5-d5793b8d2af4"
      },
      "source": [
        "#with unbalance data\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "28IqhHKgITZm",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.3843129913889929\n",
            "F1 Score : 0.4100386902032371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tVdCqHhHXS",
        "outputId": "ad08a862-19a1-4c5b-c4b3-af15c904d0e3"
      },
      "source": [
        "#With a dropout layer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "u5tVdCqHhHXS",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.5426042050903725\n",
            "F1 Score : 0.3649150698835872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfWdrFofVCVv",
        "outputId": "fb86dc6a-1f71-44c1-ed6b-e0bc974ae9b3"
      },
      "source": [
        "#With BiLSTM and without Dropout\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "qfWdrFofVCVv",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.5426042050903725\n",
            "F1 Score : 0.3649150698835872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kN3MLxglwou",
        "outputId": "92861b02-7220-4f49-8c92-0dbfc16752c7"
      },
      "source": [
        "#with 100 units GRU and a Flatten layer after\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "9kN3MLxglwou",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.562\n",
            "F1 Score : 0.525197133117069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gGx4vicwGTB",
        "outputId": "43907c3d-7b84-4895-b8a8-39bd988a1c36"
      },
      "source": [
        "#with 100 units biLSTM and and a flatten after\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "4gGx4vicwGTB",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.5582857142857143\n",
            "F1 Score : 0.5314962433635517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCvgYPYf30_6",
        "outputId": "0b16341a-2fd4-4f1b-d4b4-b9d0900630dc"
      },
      "source": [
        "#with an attention layer and a concatenate after 100 units biLSTM and before the dense\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "LCvgYPYf30_6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.5682857142857143\n",
            "F1 Score : 0.5367880042686917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRfaqZBcrDX3",
        "outputId": "4a68140b-bae3-4253-806e-2c3d8b999cef"
      },
      "source": [
        "#The Goemotion Data\n",
        "#without attention layer and with 100 units biLSTM and before the dense\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "fRfaqZBcrDX3",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.3878697117184575\n",
            "F1 Score : 0.414852757566727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiLramhNtSBM",
        "outputId": "816fec00-efbd-49a1-b16c-83a5ebcedcda"
      },
      "source": [
        "#The Goemotion Data\n",
        "#with an 10 attention layer and a concatenate after 100 units biLSTM and before the dense with SIGMOID RECURRENT IN LSTM\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "BiLramhNtSBM",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.39910146012729314\n",
            "F1 Score : 0.4344495694245672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW1ZyN6-WFQY",
        "outputId": "1dd1da95-6205-4406-d45b-bcc468a91706"
      },
      "source": [
        "#The Goemotion Data\n",
        "#with an 30 attention layer and a concatenate after 100 units biLSTM and before the dense with SIGMOID RECURRENT IN LSTM with 10 epochs\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "aW1ZyN6-WFQY",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.42156495694496443\n",
            "F1 Score : 0.4590584387740383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiDq8qKMt-MP",
        "outputId": "43bbca92-f4de-421e-8bf8-7e40f8979f12"
      },
      "source": [
        "#The GoEmotion Data\n",
        "#with 40 attention layer and a concatenate after 100 units biLSTM and a 0.3 Dropout before the dense\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "EiDq8qKMt-MP",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.4141812042768711\n",
            "F1 Score : 0.4556923688754325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwlJgIyZYyPf",
        "outputId": "7cdae1ef-bd27-4a02-a18e-d1dd35eeae18"
      },
      "source": [
        "#The GoEmotion Data\n",
        "#with 30 attention layer and a concatenate after 100 units biLSTM and before the dense. 6 Epochs without overampling\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy ', accuracy)\n",
        "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
        "print('F1 Score :', f1)"
      ],
      "id": "DwlJgIyZYyPf",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy  0.38960795347964733\n",
            "F1 Score : 0.43284507554545637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "executive-symphony"
      },
      "source": [
        "# Save the weights\n",
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/theSavedWeights/FinalCheckpoints')"
      ],
      "id": "executive-symphony",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laughing-blues"
      },
      "source": [
        "# Model Inference"
      ],
      "id": "laughing-blues"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd6J2zCvwGlZ",
        "outputId": "ac7be347-4b6e-4575-cdd8-22e97f29bd9c"
      },
      "source": [
        "!pip install emoji\n",
        "!pip install contractions\n",
        "# Text processing libraries\n",
        "import emoji\n",
        "import re\n",
        "import contractions\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "\n",
        "neme_entity = en_core_web_sm.load()"
      ],
      "id": "Wd6J2zCvwGlZ",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.5.0.tar.gz (185 kB)\n",
            "\u001b[?25l\r",
            "\u001b[K     |█▊                              | 10 kB 25.0 MB/s eta 0:00:01\r",
            "\u001b[K     |███▌                            | 20 kB 29.0 MB/s eta 0:00:01\r",
            "\u001b[K     |█████▎                          | 30 kB 27.2 MB/s eta 0:00:01\r",
            "\u001b[K     |███████                         | 40 kB 20.6 MB/s eta 0:00:01\r",
            "\u001b[K     |████████▉                       | 51 kB 15.1 MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▋                     | 61 kB 11.1 MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▍                   | 71 kB 12.3 MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████                  | 81 kB 13.5 MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▉                | 92 kB 12.2 MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▋              | 102 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▍            | 112 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▏          | 122 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████         | 133 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▊       | 143 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▍     | 153 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▏   | 163 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████  | 174 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▊| 184 kB 12.9 MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████████| 185 kB 12.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.5.0-py3-none-any.whl size=187457 sha256=3e846e81a035cd67735eaf827089576ced9b10d60c6e77d95093aaed6b6ef393\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b5/f6/b39abf14e94b3d6640613bbe630a66c10ccf7a12882d064fb5\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.5.0\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.52)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onm_AkIYwHln"
      },
      "source": [
        "# Building a preprocessing function to clean text\n",
        "def preprocess_corpus(x):\n",
        "\n",
        "  # Adding a space between words and punctation\n",
        "  x = re.sub( r'([a-zA-Z\\[\\]])([,;.!?])', r'\\1 \\2', x)\n",
        "  x = re.sub( r'([,;.!?])([a-zA-Z\\[\\]])', r'\\1 \\2', x)\n",
        "\n",
        "  # Demojize\n",
        "  x = emoji.demojize(x)\n",
        "\n",
        "  # Expand contraction\n",
        "  x = contractions.fix(x)\n",
        "\n",
        "  # Lower\n",
        "  x = x.lower()\n",
        "\n",
        "  #correct some acronyms/typos/abbreviations\n",
        "  x = re.sub(r\"lmao\", \"laughing my ass off\", x)\n",
        "  x = re.sub(r\"amirite\", \"am i right\", x)\n",
        "  x = re.sub(r\"\\b(tho)\\b\", \"though\", x)\n",
        "  x = re.sub(r\"\\b(ikr)\\b\", \"i know right\", x)\n",
        "  x = re.sub(r\"\\b(ya|u)\\b\", \"you\", x)\n",
        "  x = re.sub(r\"\\b(eu)\\b\", \"europe\", x)\n",
        "  x = re.sub(r\"\\b(da)\\b\", \"the\", x)\n",
        "  x = re.sub(r\"\\b(dat)\\b\", \"that\", x)\n",
        "  x = re.sub(r\"\\b(dats)\\b\", \"that is\", x)\n",
        "  x = re.sub(r\"\\b(cuz)\\b\", \"because\", x)\n",
        "  x = re.sub(r\"\\b(fkn)\\b\", \"fucking\", x)\n",
        "  x = re.sub(r\"\\b(tbh)\\b\", \"to be honest\", x)\n",
        "  x = re.sub(r\"\\b(tbf)\\b\", \"to be fair\", x)\n",
        "  x = re.sub(r\"faux pas\", \"mistake\", x)\n",
        "  x = re.sub(r\"\\b(btw)\\b\", \"by the way\", x)\n",
        "  x = re.sub(r\"\\b(bs)\\b\", \"bullshit\", x)\n",
        "  x = re.sub(r\"\\b(kinda)\\b\", \"kind of\", x)\n",
        "  x = re.sub(r\"\\b(bruh)\\b\", \"bro\", x)\n",
        "  x = re.sub(r\"\\b(w/e)\\b\", \"whatever\", x)\n",
        "  x = re.sub(r\"\\b(w/)\\b\", \"with\", x)\n",
        "  x = re.sub(r\"\\b(w/o)\\b\", \"without\", x)\n",
        "  x = re.sub(r\"\\b(doj)\\b\", \"department of justice\", x)\n",
        "\n",
        "  #replace some words with multiple occurences of a letter, example \"coooool\" turns into --> cool\n",
        "  x = re.sub(r\"\\b(j+e{2,}z+e*)\\b\", \"jeez\", x)\n",
        "  x = re.sub(r\"\\b(co+l+)\\b\", \"cool\", x)\n",
        "  x = re.sub(r\"\\b(g+o+a+l+)\\b\", \"goal\", x)\n",
        "  x = re.sub(r\"\\b(s+h+i+t+)\\b\", \"shit\", x)\n",
        "  x = re.sub(r\"\\b(o+m+g+)\\b\", \"omg\", x)\n",
        "  x = re.sub(r\"\\b(w+t+f+)\\b\", \"wtf\", x)\n",
        "  x = re.sub(r\"\\b(w+h+a+t+)\\b\", \"what\", x)\n",
        "  x = re.sub(r\"\\b(y+e+y+|y+a+y+|y+e+a+h+)\\b\", \"yeah\", x)\n",
        "  x = re.sub(r\"\\b(w+o+w+)\\b\", \"wow\", x)\n",
        "  x = re.sub(r\"\\b(w+h+y+)\\b\", \"why\", x)\n",
        "  x = re.sub(r\"\\b(s+o+)\\b\", \"so\", x)\n",
        "  x = re.sub(r\"\\b(f)\\b\", \"fuck\", x)\n",
        "  x = re.sub(r\"\\b(w+h+o+p+s+)\\b\", \"whoops\", x)\n",
        "  x = re.sub(r\"\\b(ofc)\\b\", \"of course\", x)\n",
        "  x = re.sub(r\"\\b(the us)\\b\", \"usa\", x)\n",
        "  x = re.sub(r\"\\b(gf)\\b\", \"girlfriend\", x)\n",
        "  x = re.sub(r\"\\b(hr)\\b\", \"human ressources\", x)\n",
        "  x = re.sub(r\"\\b(mh)\\b\", \"mental health\", x)\n",
        "  x = re.sub(r\"\\b(idk)\\b\", \"i do not know\", x)\n",
        "  x = re.sub(r\"\\b(gotcha)\\b\", \"i got you\", x)\n",
        "  x = re.sub(r\"\\b(y+e+p+)\\b\", \"yes\", x)\n",
        "  x = re.sub(r\"\\b(a*ha+h[ha]*|a*ha +h[ha]*)\\b\", \"haha\", x)\n",
        "  x = re.sub(r\"\\b(o?l+o+l+[ol]*)\\b\", \"lol\", x)\n",
        "  x = re.sub(r\"\\b(o*ho+h[ho]*|o*ho +h[ho]*)\\b\", \"ohoh\", x)\n",
        "  x = re.sub(r\"\\b(o+h+)\\b\", \"oh\", x)\n",
        "  x = re.sub(r\"\\b(a+h+)\\b\", \"ah\", x)\n",
        "  x = re.sub(r\"\\b(u+h+)\\b\", \"uh\", x)\n",
        "\n",
        "  # Handling emojis\n",
        "  x = re.sub(r\"<3\", \" love \", x)\n",
        "  x = re.sub(r\"xd\", \" smiling_face_with_open_mouth_and_tightly_closed_eyes \", x)\n",
        "  x = re.sub(r\":\\)\", \" smiling_face \", x)\n",
        "  x = re.sub(r\"^_^\", \" smiling_face \", x)\n",
        "  x = re.sub(r\"\\*_\\*\", \" star_struck \", x)\n",
        "  x = re.sub(r\":\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\":\\^\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\";\\(\", \" frowning_face \", x)\n",
        "  x = re.sub(r\":\\/\",  \" confused_face\", x)\n",
        "  x = re.sub(r\";\\)\",  \" wink\", x)\n",
        "  x = re.sub(r\">__<\",  \" unamused \", x)\n",
        "  x = re.sub(r\"\\b([xo]+x*)\\b\", \" xoxo \", x)\n",
        "  x = re.sub(r\"\\b(n+a+h+)\\b\", \"no\", x)\n",
        "\n",
        "  # Handling special cases of text\n",
        "  x = re.sub(r\"h a m b e r d e r s\", \"hamberders\", x)\n",
        "  x = re.sub(r\"b e n\", \"ben\", x)\n",
        "  x = re.sub(r\"s a t i r e\", \"satire\", x)\n",
        "  x = re.sub(r\"y i k e s\", \"yikes\", x)\n",
        "  x = re.sub(r\"s p o i l e r\", \"spoiler\", x)\n",
        "  x = re.sub(r\"thankyou\", \"thank you\", x)\n",
        "  x = re.sub(r\"a^r^o^o^o^o^o^o^o^n^d\", \"around\", x)\n",
        "\n",
        "  # Remove special characters and numbers replace by space + remove double space\n",
        "  x = re.sub(r\"\\b([.]{3,})\",\" dots \", x)\n",
        "  x = re.sub(r\"[^A-Za-z!?_]+\",\" \", x)\n",
        "  x = re.sub(r\"\\b([s])\\b *\",\"\", x)\n",
        "  x = re.sub(r\" +\",\" \", x)\n",
        "\n",
        "  # name entity\n",
        "  # neme_entity = en_core_web_sm.load()\n",
        "  n_e = neme_entity(x)\n",
        "  n_e = list(map(str, list(n_e.ents)))\n",
        "\n",
        "  # Lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  filter_words = ['name']\n",
        "  negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']\n",
        "  stop_words = [z for z in STOP_WORDS if z not in negative]\n",
        "  preprocessed_tokens = [lemmatizer.lemmatize(temp) for temp in x.split() if temp not in (filter_words + stop_words + n_e)]\n",
        "  x = ' '.join([pt for pt in preprocessed_tokens]).strip()\n",
        "\n",
        "  return x"
      ],
      "id": "onm_AkIYwHln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RPIDDqMwJvT"
      },
      "source": [
        "def roberta_inference_encode(data,maximum_length) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    encoded = tokenizer.encode_plus(\n",
        "    data,\n",
        "    add_special_tokens=True,\n",
        "    max_length=maximum_length,\n",
        "    pad_to_max_length=True,\n",
        "\n",
        "    return_attention_mask=True\n",
        "\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)"
      ],
      "id": "4RPIDDqMwJvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knaGn866wM8_"
      },
      "source": [
        "def predict_samples(text_samples, threshold):\n",
        "\n",
        "    preprocessed_text = preprocess_corpus(text_samples)\n",
        "    input_ids, attention_masks = roberta_inference_encode(preprocessed_text, maximum_length = max_len)\n",
        "    model = create_model(roberta_model, max_len)\n",
        "    model.load_weights('/content/drive/MyDrive/Colab Notebooks/theSavedWeights/FinalCheckpoints')\n",
        "    result = model.predict([input_ids, attention_masks])\n",
        "    darsad = []\n",
        "    for s in result:\n",
        "      darsad.append(s)\n",
        "\n",
        "    # Label prediction using threshold\n",
        "    samples_pred_labels = proba_to_labels(result)\n",
        "\n",
        "    # if no predictions ==> neutral\n",
        "    for pred in samples_pred_labels:\n",
        "        if pred.sum()==0:\n",
        "            pred[-1]=1\n",
        "\n",
        "    return pd.DataFrame(dict(zip( Lable_names , [ x*100 for x in result[0]])).items(), columns = ['Category', '%'])\n",
        "    # samples_pred_labels_df = pd.DataFrame(samples_pred_labels)\n",
        "    # samples_pred_labels_df = samples_pred_labels_df.apply(lambda x: [Lable_names[i] for i in range(len(x)) if x[i]==1], axis=1)\n",
        "\n",
        "    # #return list(samples_pred_labels_df)\n",
        "    # return pd.DataFrame({\"Text\":text_samples, \"Emotions\":list(samples_pred_labels_df)})"
      ],
      "id": "knaGn866wM8_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UeqYsUuTwPHt",
        "outputId": "1161556a-2513-4e92-8f24-9ba71bf672a7"
      },
      "source": [
        "predict_samples(\"My favourite food is anything I didn't have to cook myself\", threshold_opt)"
      ],
      "id": "UeqYsUuTwPHt",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admiration</td>\n",
              "      <td>4.020154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amusement</td>\n",
              "      <td>0.531616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>0.743625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>annoyance</td>\n",
              "      <td>1.198156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>approval</td>\n",
              "      <td>7.418519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>caring</td>\n",
              "      <td>1.019605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>confusion</td>\n",
              "      <td>3.461461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>curiosity</td>\n",
              "      <td>0.977627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>desire</td>\n",
              "      <td>1.231707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>disappointment</td>\n",
              "      <td>1.357310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>disapproval</td>\n",
              "      <td>2.913022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>disgust</td>\n",
              "      <td>0.862150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>embarrassment</td>\n",
              "      <td>0.506827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>excitement</td>\n",
              "      <td>1.564630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fear</td>\n",
              "      <td>0.642592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gratitude</td>\n",
              "      <td>0.565950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>grief</td>\n",
              "      <td>0.474879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>joy</td>\n",
              "      <td>2.750963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>love</td>\n",
              "      <td>56.527817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>nervousness</td>\n",
              "      <td>0.666798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>optimism</td>\n",
              "      <td>1.658364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>pride</td>\n",
              "      <td>0.852565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>realization</td>\n",
              "      <td>1.485493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>relief</td>\n",
              "      <td>0.491358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>remorse</td>\n",
              "      <td>0.538861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sadness</td>\n",
              "      <td>0.500889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>surprise</td>\n",
              "      <td>0.319426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>neutral</td>\n",
              "      <td>4.717640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Category          %\n",
              "0       admiration   4.020154\n",
              "1        amusement   0.531616\n",
              "2            anger   0.743625\n",
              "3        annoyance   1.198156\n",
              "4         approval   7.418519\n",
              "5           caring   1.019605\n",
              "6        confusion   3.461461\n",
              "7        curiosity   0.977627\n",
              "8           desire   1.231707\n",
              "9   disappointment   1.357310\n",
              "10     disapproval   2.913022\n",
              "11         disgust   0.862150\n",
              "12   embarrassment   0.506827\n",
              "13      excitement   1.564630\n",
              "14            fear   0.642592\n",
              "15       gratitude   0.565950\n",
              "16           grief   0.474879\n",
              "17             joy   2.750963\n",
              "18            love  56.527817\n",
              "19     nervousness   0.666798\n",
              "20        optimism   1.658364\n",
              "21           pride   0.852565\n",
              "22     realization   1.485493\n",
              "23          relief   0.491358\n",
              "24         remorse   0.538861\n",
              "25         sadness   0.500889\n",
              "26        surprise   0.319426\n",
              "27         neutral   4.717640"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lulmcx70wRUB",
        "outputId": "cdbe465d-0e31-4570-9383-2d26913defd4"
      },
      "source": [
        "predict_samples(\"a dog in the park\", threshold_opt)"
      ],
      "id": "lulmcx70wRUB",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:5 out of the last 344 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f406ed7c170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admiration</td>\n",
              "      <td>1.182116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amusement</td>\n",
              "      <td>0.836808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>1.904270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>annoyance</td>\n",
              "      <td>1.698164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>approval</td>\n",
              "      <td>3.284207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>caring</td>\n",
              "      <td>0.505274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>confusion</td>\n",
              "      <td>0.832113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>curiosity</td>\n",
              "      <td>0.782153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>desire</td>\n",
              "      <td>0.976796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>disappointment</td>\n",
              "      <td>1.024246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>disapproval</td>\n",
              "      <td>1.046590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>disgust</td>\n",
              "      <td>1.039706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>embarrassment</td>\n",
              "      <td>0.475029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>excitement</td>\n",
              "      <td>1.770115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fear</td>\n",
              "      <td>0.420377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gratitude</td>\n",
              "      <td>0.223886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>grief</td>\n",
              "      <td>0.536492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>joy</td>\n",
              "      <td>1.334897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>love</td>\n",
              "      <td>0.779830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>nervousness</td>\n",
              "      <td>0.856615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>optimism</td>\n",
              "      <td>1.007832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>pride</td>\n",
              "      <td>0.947817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>realization</td>\n",
              "      <td>1.665314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>relief</td>\n",
              "      <td>0.770144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>remorse</td>\n",
              "      <td>0.381997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sadness</td>\n",
              "      <td>0.465740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>surprise</td>\n",
              "      <td>0.328638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>neutral</td>\n",
              "      <td>72.922826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Category          %\n",
              "0       admiration   1.182116\n",
              "1        amusement   0.836808\n",
              "2            anger   1.904270\n",
              "3        annoyance   1.698164\n",
              "4         approval   3.284207\n",
              "5           caring   0.505274\n",
              "6        confusion   0.832113\n",
              "7        curiosity   0.782153\n",
              "8           desire   0.976796\n",
              "9   disappointment   1.024246\n",
              "10     disapproval   1.046590\n",
              "11         disgust   1.039706\n",
              "12   embarrassment   0.475029\n",
              "13      excitement   1.770115\n",
              "14            fear   0.420377\n",
              "15       gratitude   0.223886\n",
              "16           grief   0.536492\n",
              "17             joy   1.334897\n",
              "18            love   0.779830\n",
              "19     nervousness   0.856615\n",
              "20        optimism   1.007832\n",
              "21           pride   0.947817\n",
              "22     realization   1.665314\n",
              "23          relief   0.770144\n",
              "24         remorse   0.381997\n",
              "25         sadness   0.465740\n",
              "26        surprise   0.328638\n",
              "27         neutral  72.922826"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yxPg_TcHwTX_",
        "outputId": "a1d49799-2402-457c-b7b5-1f33c0df5679"
      },
      "source": [
        "predict_samples(\"are you kiddin me ??!!\", threshold_opt)"
      ],
      "id": "yxPg_TcHwTX_",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:6 out of the last 345 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f406ef1d170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admiration</td>\n",
              "      <td>0.628410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>amusement</td>\n",
              "      <td>1.225294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>2.950446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>annoyance</td>\n",
              "      <td>2.261694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>approval</td>\n",
              "      <td>2.097161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>caring</td>\n",
              "      <td>0.643231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>confusion</td>\n",
              "      <td>22.178718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>curiosity</td>\n",
              "      <td>33.237451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>desire</td>\n",
              "      <td>0.301342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>disappointment</td>\n",
              "      <td>1.141976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>disapproval</td>\n",
              "      <td>1.421469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>disgust</td>\n",
              "      <td>1.212188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>embarrassment</td>\n",
              "      <td>0.765484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>excitement</td>\n",
              "      <td>1.971850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fear</td>\n",
              "      <td>0.717922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gratitude</td>\n",
              "      <td>0.420025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>grief</td>\n",
              "      <td>0.813786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>joy</td>\n",
              "      <td>0.602622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>love</td>\n",
              "      <td>0.663108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>nervousness</td>\n",
              "      <td>0.835065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>optimism</td>\n",
              "      <td>1.013334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>pride</td>\n",
              "      <td>0.394995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>realization</td>\n",
              "      <td>1.586969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>relief</td>\n",
              "      <td>0.663517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>remorse</td>\n",
              "      <td>0.480902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sadness</td>\n",
              "      <td>0.618911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>surprise</td>\n",
              "      <td>6.413799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>neutral</td>\n",
              "      <td>12.738332</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Category          %\n",
              "0       admiration   0.628410\n",
              "1        amusement   1.225294\n",
              "2            anger   2.950446\n",
              "3        annoyance   2.261694\n",
              "4         approval   2.097161\n",
              "5           caring   0.643231\n",
              "6        confusion  22.178718\n",
              "7        curiosity  33.237451\n",
              "8           desire   0.301342\n",
              "9   disappointment   1.141976\n",
              "10     disapproval   1.421469\n",
              "11         disgust   1.212188\n",
              "12   embarrassment   0.765484\n",
              "13      excitement   1.971850\n",
              "14            fear   0.717922\n",
              "15       gratitude   0.420025\n",
              "16           grief   0.813786\n",
              "17             joy   0.602622\n",
              "18            love   0.663108\n",
              "19     nervousness   0.835065\n",
              "20        optimism   1.013334\n",
              "21           pride   0.394995\n",
              "22     realization   1.586969\n",
              "23          relief   0.663517\n",
              "24         remorse   0.480902\n",
              "25         sadness   0.618911\n",
              "26        surprise   6.413799\n",
              "27         neutral  12.738332"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTzb9ifwXu3"
      },
      "source": [
        "------------------------------------------------------------"
      ],
      "id": "foTzb9ifwXu3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrative-department"
      },
      "source": [
        "def plot_result(result):\n",
        "    sns.barplot(x = 'Category', y = 'Confidence', data = result)\n",
        "    plt.xlabel('Categories', size=14)\n",
        "    plt.ylabel('Confidence', size=14)\n",
        "    plt.title('Emotion Classification', size=16)"
      ],
      "id": "narrative-department",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpha-victory"
      },
      "source": [
        "def roberta_inference_encode(data,maximum_length) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    encoded = tokenizer.encode_plus(\n",
        "    data,\n",
        "    add_special_tokens=True,\n",
        "    max_length=maximum_length,\n",
        "    pad_to_max_length=True,\n",
        "\n",
        "    return_attention_mask=True\n",
        "\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)"
      ],
      "id": "alpha-victory",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "separate-company"
      },
      "source": [
        "def inference(text_sentence, max_len):\n",
        "    preprocessed_text = preprocess(text_sentence)\n",
        "    input_ids, attention_masks = roberta_inference_encode(preprocessed_text, maximum_length = max_len)\n",
        "    model = create_model(roberta_model, 12)\n",
        "    model.load_weights('/content/drive/MyDrive/GoEmotion CleanData and Code/theSavedWeights/FinalCheckpoints')\n",
        "    result = model.predict([input_ids, attention_masks])\n",
        "    #le.categories_[0] = ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n",
        "    result = pd.DataFrame(dict(zip(list(le.categories_[0]), [round(x*100, 2)for x in result[0]])).items(), columns = ['Category', 'Confidence'])\n",
        "    plot_result(result)\n",
        "    return result"
      ],
      "id": "separate-company",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "indian-concord",
        "outputId": "7681998e-2a79-40b5-d724-ccf22fdeb773"
      },
      "source": [
        "result = inference(\"My baby is so annoying\", max_len)\n",
        "print(result)"
      ],
      "id": "indian-concord",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "ename": "NotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /content/gdrive/MyDrive/Thesisdata/my_checkpoint",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-3e9ec453c8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My baby is so annoying\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-575f330aee53>\u001b[0m in \u001b[0;36minference\u001b[0;34m(text_sentence, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_inference_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroberta_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Thesisdata/my_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#le.categories_[0] = ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2327\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   3012\u001b[0m   \u001b[0;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m   \u001b[0;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /content/gdrive/MyDrive/Thesisdata/my_checkpoint"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNRAZpFUo_HM"
      },
      "source": [],
      "id": "nNRAZpFUo_HM",
      "execution_count": null,
      "outputs": []
    }
  ]
}